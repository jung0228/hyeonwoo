\section{Experiments}
\label{sec:experiments}

\subsection{Benchmark and Metrics}

\noindent\textbf{MLVU}$_\texttt{dev-mc}$~\citep{mlvu} is the multiple-choice subset of the MLVU-dev benchmark. It focuses on evaluating the long-form video understanding of MLLMs. The question-answer pairs are manually labeled and can be divided into 3 groups: single-detail, multi-detail, and holistic. The evaluation metric is Accuracy.

\noindent\textbf{\textsc{QaEgo4D}}$_\texttt{test-mc}$~\citep{di2023groundvqa} is the multiple-choice subset of the \textsc{QaEgo4D}-test benchmark, focusing on question-answering in long egocentric videos. It includes annotations marking video segments relevant to each question. The evaluation metric is Accuracy.

\noindent\textbf{EgoSchema}~\citep{egoschema} is a diagnostic benchmark for long VideoQA, featuring over 5000 multiple-choice questions and long temporal certificate length. It challenges AI models with long-term understanding, as current state-of-the-art models achieve significantly lower accuracy compared to human performance.

\noindent\textbf{ActivityNet-QA}~\citep{activitynet_qa} 
encompasses human-annotated QA pairs on 5,800 videos derived from the ActivityNet~\citep{activitynet} dataset. 
This benchmark is designed to assess the capabilities of VideoQA models in long-term spatiotemporal reasoning. Our evaluation methodology aligns with that of Video-ChatGPT~\citep{video_chatgpt}, employing \texttt{GPT-3.5-turbo-0613} to judge the accuracy of the open-ended VideoQA responses.

\input{tables/benchmarks}

\noindent\textbf{RVS-Ego} and \textbf{RVS-Movie}~\citep{flashvstream} are Streaming VideoQA benchmarks, constructed using 10 long videos from the Ego4D dataset~\citep{ego4d} and 22 long videos from the MovieNet dataset~\citep{movienet}, respectively. 
These benchmarks feature open-ended questions paired with timestamps, 
which are initially generated by GPT-4V~\citep{gpt4v} and GPT-4~\citep{gpt4}, and subsequently refined through manual filtering.

\noindent\textbf{CGBench}$_\texttt{mc}$~\citep{cgbench}, the multiple-choice subset of CGBench, is designed for clue-grounded question answering in long videos. It focuses on the ability to retrieve relevant clues for questions, making it an ideal testbed for~\ourmethod.

\subsection{Implementation Details}
We primarily evaluate our approach by integrating it into \texttt{LLaVA-OV-0.5B} and \texttt{LLaVA-OV-7B}~\citep{llava_onevision}, chosen for their simplicity and strong performance.
In the Appendix, we conduct experiments with several other Video-LLMs as further validations.

All experiments are conducted on NVIDIA A100 (80GB) GPUs with FP16 precision.
For video modeling, we process the video stream at 0.5 FPS, in line with GPT-4o's testing on MLVU~\citep{mlvu}. 
The local window size is set to 15K.
For external video KV-Cache retrieval, we use \texttt{SigLIP-SO400M}~\citep{siglip} as the retriever.
For internal KV-Cache retrieval, we set the block size ($b$) to 1 and the number of retrieved frames ($r$) to 64 by default, with further hyper-parameter variations explored in Section~\ref{sec:exp_ablations}.

Unless otherwise specified, \textbf{\ourmethod} refers to the use of internal video KV-Cache retrieval.

\subsection{Ablations}
\label{sec:exp_ablations}
In this section, we conduct ablation studies on the effectiveness of in-context retrieval, number of retrieved frames, and the block size.

\input{tables/qaego4d}

\textbf{Effectiveness of In-context Retrieval.}
The experiments on \textsc{QaEgo4D}$_{\texttt{test-mc}}$, as presented in Table~\ref{tab:ablation_qaego4d}, demonstrate the effects of various retrieval methods on VideoQA accuracy and recall.
The recall metric, defined as the percentage of question-relevant video frames retrieved, exhibits a strong positive correlation with VideoQA performance: higher recall consistently leads to better accuracy.
Uniform Sampling, which sparsely selects frames, achieves the lowest recall and, consequently, the poorest VideoQA accuracy. 
In contrast, Oracle Retrieval, with perfect recall, delivers the highest VideoQA accuracy, significantly outperforming Uniform Sampling.
While External and Internal Retrieval fall short of Oracle-level precision, both surpass Uniform Sampling, with Internal Retrieval excelling due to its higher recall. 

The MLVU benchmark~\citep{mlvu} encompasses three types of VideoQA tasks: \textit{Single Detail} requires identifying a single critical plot within a long video, \textit{Multi Detail} necessitates the integration of multiple plots, and \textit{Holistic} demands a comprehensive understanding of the entire video.
This makes MLVU an ideal platform for evaluating our in-context retrieval method. 
As shown in Table~\ref{tab:ablation_mlvu}, both External and Internal Retrieval enhance the overall VideoQA accuracy over the Uniform Sampling baseline. 
The enhancements are most pronounced in Single Detail tasks, 
demonstrating that \ourmethod~effectively retrieves question-relevant video context. Furthermore, Internal Retrieval significantly outperforms External Retrieval in Holistic tasks, likely due to its ability to capture broader context and leverage the Video-LLMâ€™s video modeling capabilities, as discussed in Section~\ref{sec:method}.

\textbf{Number of Retrieved Frames.}
We fix the block size ($b = 1$) and evaluate the impact of varying the numbers of retrieved frames ($r \in \{8, 16, 32, 48, 64, 80\}$) on the \textsc{QaEgo4D} and MLVU benchmarks.
As illustrated in Figure~\ref{fig:ablation_retrieval}(a), increasing the number of retrieved frames generally improves VideoQA accuracy, as it implies capturing more relevant visual context. 
However, on MLVU, this improvement plateaus as more frames are retrieved since the additional irrelevant information hinders the subsequent question-answering process. Additionally, retrieving more frames increases the computational overhead of the question-answering stage, 
further slowing down inference.

\textbf{Retrieval Block Size.}
When processing video streams, we group $b$ consecutive frames into blocks for block-level retrieval. For this experiment, we fix the number of retrieved frames at $r = 64$ and evaluate different block sizes ($b \in {1, 2, 4, 8, 16}$).
With a fixed $r$, larger block sizes result in fewer, more concentrated retrieved blocks.
Figure~\ref{fig:ablation_retrieval}(b) shows that increasing block size negatively affects accuracy on MLVU, while performance on \textsc{QaEgo4D} remains relatively stable.
This suggests that MLVU tasks benefit from retrieving more dispersed visual cues, aligning with its design of multi-detail and holistic tasks~\citep{mlvu}. 
In contrast, \textsc{QaEgo4D} primarily relies on a single relevant clip per question~\citep{di2023groundvqa}.

\input{figures/ablation_retrieval}

\input{tables/ablation_mlvu}

\subsection{Offline Video Question-answering}

\input{tables/sota}

Streaming video understanding is a relatively under-explored area, with limited StreamingVQA benchmarks available~\citep{flashvstream}.
As discussed in Section~\ref{sec:task},
OfflineVQA can be considered as a special case of StreamingVQA.
Thus, we first evaluate our method in the offline setting using four widely adopted long-form VideoQA benchmarks, comparing our results against state-of-the-art VideoQA methods.
A summary of these benchmarks can be found in Table~\ref{tab:benchmark_comparison}.

As shown in Table~\ref{tab:sota}, our proposed \ourmethod~always enhances the performance of \texttt{LLaVA-OV-0.5B} and \texttt{LLaVA-OV-7B} without additional training.
Notably, \texttt{LLaVA-OV-7B}+\ourmethod~outperforms two memory-based StreamingVQA models (VideoStreaming~\citep{videostreaming} and Flash-VStream~\citep{flashvstream}) by a large margin. 
While the base model already demonstrates strong performance, 
and we \textbf{do not} claim credit for this achievement, 
our method can integrate seamlessly with Video-LLMs, benefiting from their ongoing advancements.

\subsection{Streaming Video Question-answering} \label{sec:streaming}

\input{tables/streamingvqa}

We then evaluate our method on the streaming setting using the RVS-Ego and RVS-Movie benchmarks. During video stream modeling, questions are input immediately after their annotated end timestamps and answered based on the preceding video content.

\noindent\textbf{Question-answering Performance.}
Table~\ref{tab:streamingvqa} presents the StreamingVQA performance. 
Both external and internal retrieval methods significantly outperform the uniform sampling baseline. 
Additionally, our approach enables \texttt{LLaVA-OV-7B} to surpass Flash-VStream~\citep{flashvstream}, demonstrating \ourmethod's effectiveness for the StreamingVQA.

\textbf{Running Speed and Memory Usage.}
We also examine the running speed and memory usage under controlled conditions. 
Specifically, a 1-hour, 1080P video from RVS-Ego with 100 scattered questions is used. 
Each question is padded to 64 tokens, and the generated answers are fixed at 128 tokens in length. 
The video frames are pre-extracted at 0.5 FPS (1,800 frames in total) and streamed to the Video-LLM frame by frame. 

As illustrated in Table~\ref{tab:streamingvqa}, both retrieval methods maintain high video encoding speeds, with \texttt{LLaVA-OV-7B} achieving 11 FPS and \texttt{LLaVA-OV-0.5B} achieving 17 FPS.
Moreover, KV-Cache offloading remains manageable, with \texttt{LLaVA-OV-7B} at 18.8GB/h and \texttt{LLaVA-OV-0.5B} at 4.0GB/h (see appendix for more details).
External retrieval, however, introduces higher latency and GPU memory usage due to additional computations in the external retriever, whereas internal retrieval significantly reduces both.
Figure~\ref{fig:teaser} has also demonstrated that latency and GPU memory usage remain stable as more frames are processed.
Flash-VStream also shows good efficiency. However, it only maintains a relatively small memory footprint (681 tokens)~\citep{flashvstream}, leading to potential information loss when dealing with extremely long videos.

\textbf{Qualitative Examples.}
Figure~\ref{fig:visualization} presents an example of streaming video question-answering.
Our approach continuously processes video streams while responding to questions posed at different timestamps.
To improve efficiency, it stores and retrieves relevant video KV-Caches as contextual information for answering these questions.

We provide additional implementation details and experimental results in the Appendix.

\input{figures/visualization}
