\section{Related Work}
\label{sec:related_work}

\noindent\textbf{LLMs for Video Understanding.} 
In recent years, there has been a surge of interest in leveraging Large Language Models (LLMs) for video understanding, leading to the development of several innovative approaches~\citep{video_chatgpt, llava_next_video, llava_onevision}. These models typically use a Vision Encoder to extract video features, followed by a mapping step with Linear Projection, MLP, or Q-Former~\citep{blip2}. The mapped features are combined with textual data and fed into large language models (LLMs) to generate a text output. These models have relatively simple architectures, requiring less training data and computational resources, yet they achieve strong performance on short video understanding benchmarks~\citep{msrvtt_qa, next_qa, li2024mvbench}. However, they employ sparse sampling or token compression techniques to reduce the number of tokens, which can result in significant information loss when dealing with longer or more content-rich videos. As a result, they are not well-suited for long video understanding or streaming video understanding.

\noindent\textbf{Long Video Understanding.} 
A central challenge in long video understanding is effectively compressing the information from lengthy videos. Many approaches use language as a bridge, condensing videos into dense captions~\citep{llovi,video_recap,streaming_dvc}. While this achieves good results in some cases, compressing video content into text often leads to the loss of crucial visual details. Besides, as a pioneering approach to streaming video understanding, VideoLLM-Online~\citep{videollm_online} employs a data-centric methodology by interleaving video and text during training. In contrast, our approach is training-free, allowing seamless integration with various existing Video-LLMs to extend their StreamingVQA capabilities. Additionally, VideoLLM-Online retains only a single token per frame to handle long videos, which may result in visual information loss. Our method preserves complete visual information and leverages In-Context KV-Cache Retrieval to enhance efficiency.

Another line of research focuses on compressing long videos into a memory bank~\citep{wu2019long, wu2022memvit, wang2023memory}.
MC-ViT~\citep{mc_vit} adapts existing pretrained video transformers by fine-tuning them to attend to condensed visual memories. It relates closely to the token-pruning, merging, and memory-based video understanding methods. In comparison, we propose a training-free method specifically tailored to the StreamingVQA task. Incorporating MC-ViT into the StreamingVQA task could be an interesting avenue for future research, and we acknowledge its potential in this domain.
This approach has been integrated into Video-LLMs for streaming video understanding, as shown in works like VideoStreaming~\citep{videostreaming} and Flash-VStream~\citep{flashvstream}. These methods dynamically update the memory during video processing and utilize it for downstream tasks.
Despite their innovation, a major limitation of these methods is their failure to account for video length and information density, especially when using a fixed memory size. For example, Flash-VStream compresses both 10-second clips and hour-long videos into the same 681 tokens.
Furthermore, these methods lack interpretability, making it difficult to determine how much information is being compressed into the memory or whether relevant video information is being accurately retrieved during downstream tasks.

In pursuit of greater interpretability in long video understanding, methods such as GroundVQA~\citep{di2023groundvqa} and GeLM~\citep{chen2024groundedmultihopvideoqalongform} advocate for localizing relevant video clips while responding to user queries. Drawing inspiration from these, this work refrains from excessively condensing video information. By harnessing the causal capabilities of Video-LLMs, it preserves the entire Video KV-Cache, allowing for the retrieval of relevant information when required. This strategy effectively mitigates the substantial loss of video content while improving interpretability.

\noindent\textbf{Long Context Handling for LLMs.} 
Handling long text sequences in LLMs has been a major challenge due to high computational and memory costs, leading to training constraints on shorter sequences.
Techniques like StreamingLLM~\citep{streamingllm} and LM-Infinite~\citep{lm_infinite} use sliding window attention to process long sequences incrementally, but discard distant tokens, limiting the model’s ability to capture long-range dependencies. Recent approaches~\citep{infllm,quickllama,em_llm} address this by storing and retrieving previously computed KV-Caches, enabling better recall of distant contexts.

\noindent\textbf{Retrieval-Augmented Generation.} 
Retrieval-augmented generation (RAG) combines retrieval mechanisms with generative models to enhance performance across various NLP tasks by incorporating external knowledge~\citep{guu2020retrieval,lewis2020retrieval,borgeaud2022improving} and improving performance in vision-language tasks~\citep{xu2024retrieval}. In-context retrieval, recently proposed for handling long inputs~\citep{ram2023context}, retrieves information from the input document itself rather than an external knowledge base. In-context KV-Cache retrieval further improves efficiency by pre-encoding long documents, avoiding redundant encodings, and leveraging the LLM’s own retrieval capabilities for faster, more effective performance.