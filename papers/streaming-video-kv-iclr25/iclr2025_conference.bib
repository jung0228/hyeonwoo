@inproceedings{di2023groundvqa,
  title={Grounded Question-Answering in Long Egocentric Videos},
  author={Di, Shangzhe and Xie, Weidi},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{chen2024groundedmultihopvideoqalongform,
  title={Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos}, 
  author={Qirui Chen and Shangzhe Di and Weidi Xie},
  booktitle={AAAI},
  year={2025}
}

@article{mlvu,
  title={MLVU: A Comprehensive Benchmark for Multi-Task Long Video Understanding},
  author={Zhou, Junjie and Shu, Yan and Zhao, Bo and Wu, Boya and Xiao, Shitao and Yang, Xi and Xiong, Yongping and Zhang, Bo and Huang, Tiejun and Liu, Zheng},
  journal={arXiv preprint arXiv:2406.04264},
  year={2024}
}

@inproceedings{activitynet_qa,
  title={Activitynet-qa: A dataset for understanding complex web videos via question answering},
  author={Yu, Zhou and Xu, Dejing and Yu, Jun and Yu, Ting and Zhao, Zhou and Zhuang, Yueting and Tao, Dacheng},
  booktitle={AAAI},
  year={2019}
}

@inproceedings{activitynet,
  title={Activitynet: A large-scale video benchmark for human activity understanding},
  author={Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle={CVPR},
  year={2015}
}

@inproceedings{something,
  title={The" something something" video database for learning and evaluating visual common sense},
  author={Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  booktitle={ICCV},
  year={2017}
}

@article{kinetics,
  title={The kinetics human action video dataset},
  author={Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and others},
  journal={arXiv:1705.06950},
  year={2017}
}


@inproceedings{llamavid,
  title={LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models},
  author={Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
  booktitle={ECCV},
  year={2024}
}

@inproceedings{video_chatgpt,
    title={Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
    author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
    booktitle={ACL},
    year={2024}
}

@misc{llava_next_video,
  title={LLaVA-NeXT: A Strong Zero-shot Video Understanding Model},
  url={https://llava-vl.github.io/blog/2024-04-30-llava-next-video/},
  author={Zhang, Yuanhan and Li, Bo and Liu, haotian and Lee, Yong jae and Gui, Liangke and Fu, Di and Feng, Jiashi and Liu, Ziwei and Li, Chunyuan},
  month={April},
  year={2024}
}

@article{llava_onevision,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Li, Yanwei and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@misc{gpt4,
  title={GPT-4},
  url={https://cdn.openai.com/papers/gpt-4-system-card.pdf},
  author={OpenAI},
  month={March},
  year={2023}
}

@misc{gpt4v,
  title={GPT-4V},
  url={https://openai.com/index/gpt-4v-system-card/},
  author={OpenAI},
  month={September},
  year={2023}
}

@misc{gpt4o,
  title={GPT-4o},
  url={https://openai.com/index/hello-gpt-4o/},
  author={OpenAI},
  month={May},
  year={2024}
}

@article{flashvstream,
  title={Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams}, 
  author={Haoji Zhang and Yiqin Wang and Yansong Tang and Yong Liu and Jiashi Feng and Jifeng Dai and Xiaojie Jin},
  journal={arXiv preprint arXiv:2406.08085},
  year={2024}
}

@inproceedings{videostreaming,
  title={Streaming long video understanding with large language models},
  author={Qian, Rui and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Ding, Shuangrui and Lin, Dahua and Wang, Jiaqi},
  booktitle={NeurIPS},
  year={2024}
}


@inproceedings{liu2024world,
  title={World model on million-length video and language with blockwise ringattention},
  author={Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
  booktitle={ICLR},
  year={2025}
}

@article{longva,
  title={Long context transfer from language to vision},
  author={Zhang, Peiyuan and Zhang, Kaichen and Li, Bo and Zeng, Guangtao and Yang, Jingkang and Zhang, Yuanhan and Wang, Ziyue and Tan, Haoran and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2406.16852},
  year={2024}
}

@inproceedings{siglip,
  title={Sigmoid loss for language image pre-training},
  author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={ICCV},
  year={2023}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  year={2021}
}

@inproceedings{infllm,
  title={InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory},
  author={Xiao, Chaojun and Zhang, Pengle and Han, Xu and Xiao, Guangxuan and Lin, Yankai and Zhang, Zhengyan and Liu, Zhiyuan and Sun, Maosong},
  booktitle={ICML Workshop},
  year={2024}
}

@inproceedings{quickllama,
  title={QuickLLaMA: Query-aware Inference Acceleration for Large Language Models},
  author={Li, Jingyao and Shi, Han and Jiang, Xin and Li, Zhenguo and Xu, Hong and Jia, Jiaya},
  booktitle={COLING},
  year={2025}
}

@inproceedings{em_llm,
  title={Human-like Episodic Memory for Infinite Context LLMs},
  author={Fountas, Zafeirios and Benfeghoul, Martin A and Oomerjee, Adnan and Christopoulou, Fenia and Lampouras, Gerasimos and Bou-Ammar, Haitham and Wang, Jun},
  booktitle={ICLR},
  year={2025}
}

@inproceedings{streamingllm,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  booktitle={ICLR},
  year={2024}
}

@article{lm_infinite,
  title={Lm-infinite: Simple on-the-fly length generalization for large language models},
  author={Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong},
  journal={arXiv preprint arXiv:2308.16137},
  year={2023}
}

@inproceedings{snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  booktitle={NeurIPS},
  year={2024}
}

@inproceedings{blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={ICML},
  year={2023}
}

## MSRVTT-QA & MSVD-QA
@inproceedings{msrvtt_qa,
    author = {Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
    title = {Video Question Answering via Gradually Refined Attention over Appearance and Motion},
    year = {2017},
    booktitle = {ACM Multimedia}
}

@inproceedings{jang2017tgifqa,
title={TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering}, 
author={Yunseok Jang and Yale Song and Youngjae Yu and Youngjin Kim and Gunhee Kim},
booktitle={CVPR},
year={2017},
}

## NExtQA
@inproceedings{next_qa,
    title={NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions}, 
    author={Junbin Xiao and Xindi Shang and Angela Yao and Tat-Seng Chua},
    booktitle={CVPR},
    year={2021},
}

## MVBench
@inproceedings{li2024mvbench,
  title={Mvbench: A comprehensive multi-modal video understanding benchmark},
  author={Li, Kunchang and Wang, Yali and He, Yinan and Li, Yizhuo and Wang, Yi and Liu, Yi and Wang, Zun and Xu, Jilan and Chen, Guo and Luo, Ping and others},
  booktitle={CVPR},
  year={2024}
}

@article{llovi,
  title={A simple llm framework for long-range video question-answering},
  author={Zhang, Ce and Lu, Taixi and Islam, Md Mohaiminul and Wang, Ziyang and Yu, Shoubin and Bansal, Mohit and Bertasius, Gedas},
  journal={arXiv preprint arXiv:2312.17235},
  year={2023}
}

@inproceedings{video_recap,
  title={Video ReCap: Recursive Captioning of Hour-Long Videos},
  author={Islam, Md Mohaiminul and Ho, Ngan and Yang, Xitong and Nagarajan, Tushar and Torresani, Lorenzo and Bertasius, Gedas},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{streaming_dvc,
  title={Streaming dense video captioning},
  author={Zhou, Xingyi and Arnab, Anurag and Buch, Shyamal and Yan, Shen and Myers, Austin and Xiong, Xuehan and Nagrani, Arsha and Schmid, Cordelia},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{wu2019long,
  title={Long-term feature banks for detailed video understanding},
  author={Wu, Chao-Yuan and Feichtenhofer, Christoph and Fan, Haoqi and He, Kaiming and Krahenbuhl, Philipp and Girshick, Ross},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{wu2022memvit,
  title={Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition},
  author={Wu, Chao-Yuan and Li, Yanghao and Mangalam, Karttikeya and Fan, Haoqi and Xiong, Bo and Malik, Jitendra and Feichtenhofer, Christoph},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{wang2023memory,
  title={Memory-and-anticipation transformer for online action understanding},
  author={Wang, Jiahao and Chen, Guo and Huang, Yifei and Wang, Limin and Lu, Tong},
  booktitle={ICCV},
  year={2023}
}

@article{flashattention2,
  title={Flashattention-2: Faster attention with better parallelism and work partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@inproceedings{ainslie2023gqa,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle={EMNLP},
  year={2023}
}

@article{egoschema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={NeurIPS},
  year={2023}
}

@article{gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{minigpt4_video,
  title={MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens},
  author={Ataallah, Kirolos and Shen, Xiaoqian and Abdelrahman, Eslam and Sleiman, Essam and Zhu, Deyao and Ding, Jian and Elhoseiny, Mohamed},
  booktitle={CVPR Workshop},
  year={2024}
}

@inproceedings{ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={CVPR},
  year={2022}
}

@inproceedings{movienet,
  title={Movienet: A holistic dataset for movie understanding},
  author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
  booktitle={ECCV},
  year={2020}
}

@article{got_10k,
  title={Got-10k: A large high-diversity benchmark for generic object tracking in the wild},
  author={Huang, Lianghua and Zhao, Xin and Huang, Kaiqi},
  journal={TPAMI},
  year={2019}
}

@inproceedings{trackingnet,
  title={Trackingnet: A large-scale dataset and benchmark for object tracking in the wild},
  author={Muller, Matthias and Bibi, Adel and Giancola, Silvio and Alsubaihi, Salman and Ghanem, Bernard},
  booktitle={ECCV},
  year={2018}
}

@inproceedings{he2024ma,
  title={Ma-lmm: Memory-augmented large multimodal model for long-term video understanding},
  author={He, Bo and Li, Hengduo and Jang, Young Kyun and Jia, Menglin and Cao, Xuefei and Shah, Ashish and Shrivastava, Abhinav and Lim, Ser-Nam},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{frozen_bilm,
  title={Zero-shot video question answering via frozen bidirectional language models},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{fang2023eva,
  title={Eva: Exploring the limits of masked visual representation learning at scale},
  author={Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{ram2023context,
  title={In-context retrieval-augmented language models},
  author={Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  booktitle={ACL},
  year={2023}
}

@inproceedings{xu2024retrieval,
  title={Retrieval-augmented egocentric video captioning},
  author={Xu, Jilan and Huang, Yifei and Hou, Junlin and Chen, Guo and Zhang, Yuejie and Feng, Rui and Xie, Weidi},
  booktitle={CVPR},
  year={2024}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={ICML},
  year={2022}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={ICML},
  year={2020}
}

@inproceedings{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{yuan2024kv,
  title={Kv cache compression, but what must we give in return? a comprehensive benchmark of long context capable approaches},
  author={Yuan, Jiayi and Liu, Hongyi and Chuang, Yu-Neng and Li, Songchen and Wang, Guanchu and Le, Duy and Jin, Hongye and Chaudhary, Vipin and Xu, Zhaozhuo and Liu, Zirui and others},
  booktitle={EMNLP Findings},
  year={2024}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  year={2024}
}

@inproceedings{video_llava,
    title = "Video-{LL}a{VA}: Learning United Visual Representation by Alignment Before Projection",
    author = "Lin, Bin  and
      Ye, Yang  and
      Zhu, Bin  and
      Cui, Jiaxi  and
      Ning, Munan  and
      Jin, Peng  and
      Yuan, Li",
    booktitle = {EMNLP},
    year = "2024"
}

@inproceedings{mc_vit,
  title={Memory Consolidation Enables Long-Context Video Understanding},
  author={Balazevic, Ivana and Shi, Yuge and Papalampidi, Pinelopi and Chaabouni, Rahma and Koppula, Skanda and Henaff, Olivier J},
  booktitle={ICML},
  year={2024}
}

@inproceedings{videollm_online,
  title={Videollm-online: Online video large language model for streaming video},
  author={Chen, Joya and Lv, Zhaoyang and Wu, Shiwei and Lin, Kevin Qinghong and Song, Chenan and Gao, Difei and Liu, Jia-Wei and Gao, Ziteng and Mao, Dongxing and Shou, Mike Zheng},
  booktitle={CVPR},
  year={2024}
}

@misc{ye2023calflops,
  title={Calflops: A FLOPs and params calculate tool for neural networks in pytorch framework},
  author={Ye, X},
  year={2023}
}

@inproceedings{cgbench,
  title={Cg-bench: Clue-grounded question answering benchmark for long video understanding},
  author={Chen, Guo and Liu, Yicheng and Huang, Yifei and He, Yuping and Pei, Baoqi and Xu, Jilan and Wang, Yali and Lu, Tong and Wang, Limin},
  booktitle={ICLR},
  year={2025}
}
