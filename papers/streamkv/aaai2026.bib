@article{li2023blip2bl,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Junnan Li and Dongxu Li and Silvio Savarese and Steven Hoi},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023},
}
@article{li2024llavaonevision,
    title={Llava-onevision: Easy visual task transfer},
    author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
    journal={arXiv:2408.03326},
    year={2024}
}
@article{deitke2024molmo,
    title={Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models},
    author={Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and others},
    journal={arXiv:2409.17146},
    year={2024}
}
@article{Qwen2VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}
@article{zhang2024mm1,
    title={{Mm1.5: Methods, analysis \& insights from multimodal llm fine-tuning}},
    author={Zhang, Haotian and Gao, Mingfei and Gan, Zhe and Dufter, Philipp and Wenzel, Nina and Huang, Forrest and Shah, Dhruti and Du, Xianzhi and Zhang, Bowen and Li, Yanghao and others},
    journal={ICLR},
    year={2025}
}

@article{video-llama,
    title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
    author={Zhang, Hang and Li, Xin and Bing, Lidong},
    journal={arXiv:2306.02858},
    year={2023}
}
@article{video-llava,
    title={Video-llava: Learning united visual representation by alignment before projection},
    author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},
    journal={arXiv:2311.10122},
    year={2023}
}
@article{videogpt+,
    title={VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding},
    author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad},
    journal={arXiv:2406.09418},
    year={2024}
}
@misc{xu2024slowfastllavastrongtrainingfreebaseline,
      title={SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language Models}, 
      author={Mingze Xu and Mingfei Gao and Zhe Gan and Hong-You Chen and Zhengfeng Lai and Haiming Gang and Kai Kang and Afshin Dehghan},
      year={2024},
      eprint={2407.15841},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2407.15841}, 
}


@misc{zhang2024videoinstructiontuningsynthetic,
      title={Video Instruction Tuning With Synthetic Data}, 
      author={Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},
      year={2024},
      eprint={2410.02713},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.02713}, 
}

@article{zhang2025videollama,
  title={VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding},
  author={Zhang, Boqiang and Li, Kehan and Cheng, Zesen and Hu, Zhiqiang and Yuan, Yuqian and Chen, Guanzheng and Leng, Sicong and Jiang, Yuming and Zhang, Hang and Li, Xin and others},
  journal={arXiv preprint arXiv:2501.13106},
  year={2025}
}
@article{li2024videochat-flash,
  title={Videochat-flash: Hierarchical compression for long-context video modeling},
  author={Li, Xinhao and Wang, Yi and Yu, Jiashuo and Zeng, Xiangyu and Zhu, Yuhan and Huang, Haian and Gao, Jianfei and Li, Kunchang and He, Yinan and Wang, Chenting and others},
  journal={arXiv preprint arXiv:2501.00574},
  year={2024}
}

@inproceedings{clip,
    title={Learning transferable visual models from natural language supervision},
    author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
    booktitle={ICML},
    year={2021},
}
@inproceedings{siglip,
    title={Sigmoid loss for language image pre-training},
    author={Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
    booktitle={ICCV},
    year={2023}
}
@misc{tschannen2025siglip2multilingualvisionlanguage,
      title={SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features}, 
      author={Michael Tschannen and Alexey Gritsenko and Xiao Wang and Muhammad Ferjad Naeem and Ibrahim Alabdulmohsin and Nikhil Parthasarathy and Talfan Evans and Lucas Beyer and Ye Xia and Basil Mustafa and Olivier Hénaff and Jeremiah Harmsen and Andreas Steiner and Xiaohua Zhai},
      year={2025},
      eprint={2502.14786},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.14786}, 
}
@article{liu2023llava,
    title={Visual instruction tuning},
    author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    journal={NeurIPS},
    year={2023}
}

@article{yang2024qwen2,
    title={Qwen2. 5 technical report},
    author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
    journal={arXiv:2412.15115},
    year={2024}
}

@article{videomme,
    title={Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis},
    author={Fu, Chaoyou and Dai, Yuhan and Luo, Yondong and Li, Lei and Ren, Shuhuai and Zhang, Renrui and Wang, Zihan and Zhou, Chenyu and Shen, Yunhang and Zhang, Mengdan and others},
    journal={arXiv:2405.21075},
    year={2024}
}
@article{zhou2024mlvu,
    title={Mlvu: A comprehensive benchmark for multi-task long video understanding},
    author={Zhou, Junjie and Shu, Yan and Zhao, Bo and Wu, Boya and Xiao, Shitao and Yang, Xi and Xiong, Yongping and Zhang, Bo and Huang, Tiejun and Liu, Zheng},
    journal={arXiv:2406.04264},
    year={2024}
}
@misc{lin2024streamingbenchassessinggapmllms,
      title={StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding}, 
      author={Junming Lin and Zheng Fang and Chi Chen and Zihao Wan and Fuwen Luo and Peng Li and Yang Liu and Maosong Sun},
      year={2024},
      eprint={2411.03628},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.03628}, 
}
@inproceedings{videollm_online,
  title={Videollm-online: Online video large language model for streaming video},
  author={Chen, Joya and Lv, Zhaoyang and Wu, Shiwei and Lin, Kevin Qinghong and Song, Chenan and Gao, Difei and Liu, Jia-Wei and Gao, Ziteng and Mao, Dongxing and Shou, Mike Zheng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18407--18418},
  year={2024}
}

@article{qian2025dispider,
        title={Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction},
        author={Qian, Rui and Ding, Shuangrui and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Lin, Dahua and Wang, Jiaqi},
        journal={arXiv preprint arXiv:2501.03218},
        year={2025}
}
@article{vstream,
  title={Flash-vstream: Memory-based real-time understanding for long video streams},
  author={Zhang, Haoji and Wang, Yiqin and Tang, Yansong and Liu, Yong and Feng, Jiashi and Dai, Jifeng and Jin, Xiaojie},
  journal={arXiv preprint arXiv:2406.08085},
  year={2024}
}
@inproceedings{di2025rekv,
  title={Streaming Video Question-Answering with In-context Video KV-Cache Retrieval},
  author={Di, Shangzhe and Yu, Zhelun and Zhang, Guanghao and Li, Haoyuan and Cheng, Hao and Li, Bolin and He, Wanggui and Shu, Fangxun and Jiang, Hao and others},
  booktitle={ICLR},
  year={2025}
}
@misc{chen2024image,
      title={An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models}, 
      author={Liang Chen and Haozhe Zhao and Tianyu Liu and Shuai Bai and Junyang Lin and Chang Zhou and Baobao Chang},
      year={2024},
      eprint={2403.06764},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{zhang2024sparsevlm,
      title={SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference}, 
      author={Yuan Zhang and Chun-Kai Fan and Junpeng Ma and Wenzhao Zheng and Tao Huang and Kuan Cheng and Denis Gudovskiy and Tomoyuki Okuno and Yohei Nakata and Kurt Keutzer and Shanghang Zhang},
      year={2024},
      eprint={2410.04417},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.04417}, 
}
@inproceedings{
li2024snapkv,
title={Snap{KV}: {LLM} Knows What You are Looking for Before Generation},
author={Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=poE54GOq2l}
}
@inproceedings{fu2024headkv,
      title={Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning}, 
      author={Yu Fu and Zefan Cai and Abedelkadir Asi and Wayne Xiong and Yue Dong and Wen Xiao},
      booktitle={The Thirteenth International Conference on Learning Representations},
      year={2025},
      url={https://arxiv.org/abs/2410.19258}, 
}
@inproceedings{
zhang2023ho,
title={H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
author={Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Re and Clark Barrett and Zhangyang Wang and Beidi Chen},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=RkRrPp7GKO}
}

@misc{su2023roformerenhancedtransformerrotary,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.09864}, 
}

@misc{
han2024lminfinite,
title={{LM}-Infinite: Simple On-the-Fly Length Generalization for Large Language Models},
author={Chi Han and Qifan Wang and Wenhan Xiong and Yu Chen and Heng Ji and Sinong Wang},
year={2024},
url={https://openreview.net/forum?id=pOujzgHIRY}
}

@misc{xiao2024infllmtrainingfreelongcontextextrapolation,
      title={InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory}, 
      author={Chaojun Xiao and Pengle Zhang and Xu Han and Guangxuan Xiao and Yankai Lin and Zhengyan Zhang and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.04617},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.04617}, 
}

@misc{dosovitskiy2021imageworth16x16words,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}

@article{ouyang2022instructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xian and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{touvron2023llama,
    title={Llama: Open and efficient foundation language models},
    author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
    journal={arXiv:2302.13971},
    year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{openai2023gpt4,
  title = {GPT-4 Technical Report},
  author = {OpenAI},
  journal = {arXiv preprint arXiv:2303.08774},
  year = {2023}
}
@article{bai2023qwen,
  title={Qwen-72B: A Powerful Language Model at Scale},
  author={Bai, Yi and Wang, Junyang and Zhu, Zhenglong and others},
  journal={arXiv preprint arXiv:2311.15001},
  year={2023}
}


#streaming understanding
@misc{li2025lionfsfastslow,
      title={LION-FS: Fast \& Slow Video-Language Thinker as Online Video Assistant}, 
      author={Wei Li and Bing Hu and Rui Shao and Leyang Shen and Liqiang Nie},
      year={2025},
      eprint={2503.03663},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.03663}, 
}
@article{ding2025streammind,
  title={StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through Event-Gated Cognition},
  author={Ding, Xin and Wu, Hao and Yang, Yifan and Jiang, Shiqi and Bai, Donglin and Chen, Zhibo and Cao, Ting},
  journal={arXiv preprint arXiv:2503.06220},
  year={2025}
}
@misc{wu2024videollmmodefficientvideolanguagestreaming,
      title={VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation}, 
      author={Shiwei Wu and Joya Chen and Kevin Qinghong Lin and Qimeng Wang and Yan Gao and Qianli Xu and Tong Xu and Yao Hu and Enhong Chen and Mike Zheng Shou},
      year={2024},
      eprint={2408.16730},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.16730}, 
}
@misc{liu2025streamchatchattingstreamingvideo,
      title={StreamChat: Chatting with Streaming Video}, 
      author={Jihao Liu and Zhiding Yu and Shiyi Lan and Shihao Wang and Rongyao Fang and Jan Kautz and Hongsheng Li and Jose M. Alvare},
      year={2025},
      eprint={2412.08646},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.08646}, 
}


#kvcache compression
@misc{cai2025pyramidkvdynamickvcache,
      title={PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling}, 
      author={Zefan Cai and Yichi Zhang and Bofei Gao and Yuliang Liu and Yucheng Li and Tianyu Liu and Keming Lu and Wayne Xiong and Yue Dong and Junjie Hu and Wen Xiao},
      year={2025},
      eprint={2406.02069},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.02069}, 
}
@misc{yang2024pyramidinferpyramidkvcache,
      title={PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference}, 
      author={Dongjie Yang and XiaoDong Han and Yan Gao and Yao Hu and Shilin Zhang and Hai Zhao},
      year={2024},
      eprint={2405.12532},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.12532}, 
}
@misc{wang2025sparsemmheadsparsityemerges,
      title={SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs}, 
      author={Jiahui Wang and Zuyan Liu and Yongming Rao and Jiwen Lu},
      year={2025},
      eprint={2506.05344},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05344}, 
}

@article{xiao2024dynamickv,
  title={DynamicKV: Query-Aware KV Selection for Efficient and Accurate Large Language Model Inference},
  author={Xiao, Chaojun and others},
  journal={arXiv preprint arXiv:2401.15886},
  year={2024}
}



#others
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}
@misc{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.11401}, 
}
@article{liu2023promptsurvey,
  title={A survey on prompt engineering},
  author={Liu, Peng and Xie, Yuning and Li, Min and others},
  journal={arXiv preprint arXiv:2304.06550},
  year={2023}
}

#normalization
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group}
}


@article{zhu2023survey,
  title={A Survey on Multimodal Conversation Modeling},
  author={Zhu, Longyu and Wang, Rui and others},
  journal={ACM Computing Surveys (CSUR)},
  year={2023}
}

@misc{liu2023lostmiddlelanguagemodels,
      title={Lost in the Middle: How Language Models Use Long Contexts}, 
      author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
      year={2023},
      eprint={2307.03172},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.03172}, 
}

@inproceedings{chatunivi,
  title={Chat-univi: Unified visual representation empowers large language models with image and video understanding},
  author={Jin, Peng and Takanobu, Ryuichi and Zhang, Wancai and Cao, Xiaochun and Yuan, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13700--13710},
  year={2024}
}



@misc{qasim2023densevideocaptioningsurvey,
      title={Dense Video Captioning: A Survey of Techniques, Datasets and Evaluation Protocols}, 
      author={Iqra Qasim and Alexander Horsch and Dilip K. Prasad},
      year={2023},
      eprint={2311.02538},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.02538}, 
}

@misc{luo2021clip4clipempiricalstudyclip,
      title={CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval}, 
      author={Huaishao Luo and Lei Ji and Ming Zhong and Yang Chen and Wen Lei and Nan Duan and Tianrui Li},
      year={2021},
      eprint={2104.08860},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2104.08860}, 
}

@misc{alaa2024videosummarizationtechniquescomprehensive,
      title={Video Summarization Techniques: A Comprehensive Review}, 
      author={Toqa Alaa and Ahmad Mongy and Assem Bakr and Mariam Diab and Walid Gomaa},
      year={2024},
      eprint={2410.04449},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.04449}, 
}

@misc{cao2025movekdknowledgedistillationvlms,
      title={MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders}, 
      author={Jiajun Cao and Yuan Zhang and Tao Huang and Ming Lu and Qizhe Zhang and Ruichuan An and Ningning MA and Shanghang Zhang},
      year={2025},
      eprint={2501.01709},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.01709}, 
}

@misc{cao2025fastdrivevlaefficientendtoenddriving,
      title={FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning}, 
      author={Jiajun Cao and Qizhe Zhang and Peidong Jia and Xuhui Zhao and Bo Lan and Xiaoan Zhang and Zhuo Li and Xiaobao Wei and Sixiang Chen and Liyun Li and Xianming Liu and Ming Lu and Yang Wang and Shanghang Zhang},
      year={2025},
      eprint={2507.23318},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.23318}, 
}

@misc{li2025manipdreamer3dsynthesizingplausible,
      title={ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory}, 
      author={Ying Li and Xiaobao Wei and Xiaowei Chi and Yuming Li and Zhongyu Zhao and Hao Wang and Ningning Ma and Ming Lu and Shanghang Zhang},
      year={2025},
      eprint={2509.05314},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2509.05314}, 
}