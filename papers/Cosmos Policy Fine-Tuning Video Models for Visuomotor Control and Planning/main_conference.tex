\documentclass{article} %
\usepackage{main_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{xcolor}
\definecolor{nvidiagreen}{HTML}{76B900}
\usepackage{url}

\usepackage[shortlabels]{enumitem}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{graphicx}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{amsmath} 
\usepackage{url}
\usepackage{seqsplit}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=nvidiagreen,
	filecolor=magenta,      
	urlcolor=nvidiagreen,
	citecolor=nvidiagreen,
}
\usepackage{colortbl}
\usepackage[normalem]{ulem}



\usepackage{titlesec}
\titlespacing\section{0pt}{3.3pt plus 4pt minus 2pt}{3.3pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{1.5pt plus 4pt minus 2pt}{1.5pt plus 2pt minus 2pt}

\usepackage{listings}
\lstset{
    language=Python,
    numbers=left,
    numberstyle=\tiny\color{gray},
    basicstyle=\ttfamily\tiny,
    keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{red},
    commentstyle=\color{brown}\em, %
    breaklines=true,
    showstringspaces=false,
    tabsize=4
}


\title{Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning}

\newcommand{\website}{\url{https://research.nvidia.com/labs/dir/cosmos-policy/}}

\newcommand{\pl}[1]{\textcolor{red}{[PL: #1]}}
\author{
\hspace{-.3cm}\textbf{Moo Jin Kim}$^{1,2}$ \hspace{0.18cm}
\textbf{Yihuai Gao}$^{1,2}$ \hspace{0.18cm}
\textbf{Tsung-Yi Lin}$^{1}$ \hspace{0.18cm}
\textbf{Yen-Chen Lin}$^{1}$ \hspace{0.18cm}
\textbf{Yunhao Ge}$^{1}$ \hspace{0.18cm}
\textbf{Grace Lam}$^{1}$ \\[0.8ex]
\hspace{1cm}\textbf{Percy Liang}$^{2}$ \hspace{0.18cm}
\textbf{Shuran Song}$^{1,2}$ \hspace{0.18cm}
\textbf{Ming-Yu Liu}$^{1}$ \hspace{0.18cm}
\textbf{Chelsea Finn}$^{2}$ \hspace{0.18cm}
\textbf{Jinwei Gu}$^{1}$ \\[1.2ex]
\hspace{4.3cm}$^{1}$NVIDIA \hspace{0.18cm} $^{2}$Stanford University \\[0.18cm]
\large\website
}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newcommand{\outline}[1]{%
  {\color{orange}#1}%
}
\newcommand{\reviewme}{%
  {\color{red}(Moo Jin: Ready for review!)}%
}
\newcommand\mj[1]{\textcolor{magenta}{[Moo Jin: #1]}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}
\newcommand{\rebuttal}[1]{#1}  %

\iclrfinalcopy %
\begin{document}


\maketitle
\lhead{}  %



\begin{figure*}[h]
    \vspace{-0.5cm}
    \centering
    \includegraphics[width=\linewidth]{fig/cosmos_policy_figure1.jpeg}
    \caption{We present Cosmos Policy, a state-of-the-art robot policy fine-tuned from the NVIDIA Cosmos-Predict2-2B video foundation model. Cosmos Policy handles multimodal inputs and multi-view camera images and predicts (1) a robot action chunk, (2) future state (represented by robot proprioception and image observations), and (3) value (expected rewards-to-go at the future state). No architectural changes are made to the base video model, and all modalities are jointly modeled through the video diffusion learning objective.
    }
    \label{fig:figure1}
\end{figure*}


\begin{abstract}
Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO \rebuttal{and RoboCasa} simulation benchmarks (98.5\% \rebuttal{and 67.1\%} average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at \website.
\end{abstract}





\section{Introduction}


\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\footnotetext{
    \raggedright
    Correspondence to: Moo Jin Kim \href{mailto:moojink@cs.stanford.edu}{\texttt{\seqsplit{<moojink@cs.stanford.edu>}}}.
}


Large pretrained video generation models have shown impressive ability to generate physically plausible and temporally coherent videos \citep{nvidia2025cosmosworldfoundationmodel, wan2025, yang2024cogvideox, bao2024vidu, kong2024hunyuanvideo, zheng2024open}. Unlike pretrained vision-language models---which learn semantic concepts from static image-text pairs and have been popularized as robot policy backbones by recent vision-language-action (VLA) model research (\citep{brohan2023rt, kim2024openvla, intelligence2025pi, li2025cogvla})---pretrained video generation models learn temporal causality,
implicit physics, and motion patterns from millions of videos.
These spatiotemporal priors hold significant value for robotics applications. In this work, we explore how to effectively leverage video models for robotic control and how they can incorporate policy rollout data to refine their world models and enable more effective planning.



Prior works have made significant progress on adapting video models for robotic manipulation, leveraging both robot action data and ``action-less'' Internet video data to train generalizable policies and perform new tasks with small amounts of demonstrations \citep{liang2025video, zhong2025flowvla, hu2024video, liao2025genie, unifolm-wma-0, feng2025vidar, yang2025roboenvision, wang2025latent}. However, these works often require multiple training stages (e.g., video fine-tuning followed by action module training) and introduce new architectural components, such as separate action diffusers or inverse dynamics models. Other works avoid these complexities by training unified video-action models \citep{li2025unified, zhu2025unified}, but they do not leverage pretrained video models due to their custom design, limiting their ability to capitalize on the spatiotemporal priors.





In this work, we address these limitations with \textbf{Cosmos Policy}:  an effective robot policy that is adapted from a pretrained video model (Cosmos-Predict2-2B \citep{nvidia2025cosmosworldfoundationmodel}) through a single stage of post-training on robot demonstrations. Unlike prior works which carefully design separate action modules and algorithms, Cosmos Policy makes no architectural modifications and instead leverages the pretrained model's core learning mechanism to capture action distributions. Since video models are effective at modeling complex, high-dimensional, multimodal distributions and can generate temporally coherent videos with hundreds of frames, we hypothesize that their learning algorithms are well-suited for representing actions alongside other modalities. Following this reasoning, we directly fine-tune a video model to simultaneously generate robot actions, future state images, and future state values (expected total cumulative rewards), all of which we encode as latent frames within the model's latent diffusion sequence. With future state and value predictions, Cosmos Policy can use best-of-N sampling to plan by generating candidate actions, imagining their resulting future states, ranking these states by predicted value, and executing the highest-value action. This search process produces trajectories that are more likely to succeed at the task

Our main contribution is the
Cosmos Policy approach
for fine-tuning pretrained video models to incorporate different modalities that enable
visuomotor control and planning. We evaluate our method in two modes: first as a direct policy (without planning) and then with model-based planning using the future state and value predictions. As a direct policy, Cosmos Policy achieves a new state of the art in both the LIBERO \rebuttal{and RoboCasa simulation benchmarks} (98.5\% \rebuttal{and 67.1\%} average success rates, respectively), outperforming diffusion-based policies trained from scratch, video-based policies (e.g., UVA, Video Policy), and even fine-tuned VLAs (e.g., $\pi_{0.5}$, OpenVLA-OFT, CogVLA, UniVLA, DP-VLA, GR00T-N1.5).
It also achieves the highest average success rate (93.6\%) among state-of-the-art policies in challenging real-world bimanual manipulation tasks. Further, when enhanced with model-based planning,
we observe
a 12.5 percent higher task completion rate on average in two challenging real-world manipulation tasks. In these experiments, we show that Cosmos Policy can incorporate past experiences from policy rollouts to refine its world model and value function and plan more effectively. Lastly, we compare our model-based planning approach to a model-free variant and study their relative advantages.









\section{Related Work}



\textbf{Video-based robot policies.} Recent works have made great strides in leveraging video models for manipulation. Some methods first fine-tune video models on robot data and then train separate action modules to predict robot actions from generated video frames \citep{liang2025video, zhong2025flowvla, hu2024video, liao2025genie, unifolm-wma-0, feng2025vidar, yang2025roboenvision, wang2025latent, he2024learning}. Other works train unified video-action models that jointly predict future frames and actions \citep{li2025unified, zhu2025unified}, but these approaches do not leverage pretrained video models and thus do not benefit from their spatiotemporal priors. In contrast to these works, we propose a single-stage fine-tuning approach that directly adapts pretrained video models to generate actions (as well as other modalities such as robot proprioceptive state and state values) within their native latent diffusion process.

\textbf{Vision-language-action models.} State-of-the-art robotic manipulation policies increasingly leverage large pretrained backbones. Vision-language-action (VLA) models such as RT-2 \citep{brohan2023rt}, OpenVLA \citep{kim2024openvla}, $\pi_{0.5}$ \citep{intelligence2025pi}, UniVLA \citep{bu2025univla}, and CogVLA \citep{li2025cogvla} fine-tune vision-language models on large-scale robotic imitation data, achieving strong performance across diverse manipulation tasks. While these methods exhibit strong generalization to various semantic concepts unseen in robotic interaction data, they leverage pretrained models that have mostly been trained on static image-text pairs rather than videos. In contrast to these VLAs, we leverage a pretrained video model that has learned spatiotemporal dynamics and implicit physics from predicting future frames for Internet-scale datasets. We hypothesize that this different pretrained backbone can serve as a strong foundation for low-level control policies.

\textbf{World models and value functions.}  World models have been used in various ways in robotics and reinforcement learning, from classical model-predictive control to modern neural approaches. Influential works such as Dyna \citep{sutton1991dyna}, MBPO \citep{janner2019trust}, TD-MPC \citep{hansen2022temporal, hansen2023td}, and the Dreamer family of works \citep{hafner2019dream, hafner2020mastering, hafner2023mastering} demonstrate the benefits of integrating planning with learning, using learned dynamics models to improve decision making in various control tasks. Recent works have explored different paradigms: FLARE \citep{zheng2025flare} adds learnable future tokens to diffusion transformer sequences to predict compact representations of future state, SAILOR \citep{jain2025smooth} uses separate world and reward models with MPPI planning to iteratively search for better actions and refine the base policy, and Latent Policy Steering \citep{wang2025latent} pretrains world models using optical flow as an embodiment-agnostic action representation and subsequently trains a separate value function to steer the policy towards states with higher rewards. In contrast to these prior works that rely on separate modules for the policy, world model, and value function and typically train from models from scratch, we use a single unified architecture that serves simultaneously as the policy, world model, and value function and initialize from a pretrained video model.

\section{Preliminaries}
\label{sec:prelim}


\textbf{Cosmos video model.} The pretrained video model that serves as the initialization for Cosmos Policy is Cosmos-Predict2-2B-Video2World \citep{nvidia2025cosmosworldfoundationmodel}, a latent video diffusion model that receives a starting image and textual description as input and predicts subsequent frames to create a short video.
The model operates over continuous tokens encoded by the Wan2.1 spatiotemporal VAE tokenizer \citep{wan2025} and is trained using the EDM denoising score matching formulation \citep{karras2022elucidating}. The core training objective for the denoiser network $D_\theta$ at noise level $\sigma$ is:
$ \mathcal{L}(D_\theta, \sigma) = \mathbb{E}_{\mathbf{x}_0, \mathbf{c}, \mathbf{n}}\left[\left\|D_\theta(\mathbf{x}_0 + \mathbf{n}; \sigma, \mathbf{c}) - \mathbf{x}_0\right\|_2^2\right]$,
where $\mathbf{x}_0$ is a clean VAE-encoded image sequence, $\mathbf{c}$ represents the textual description encoded as T5-XXL embeddings \citep{raffel2020exploring}, $\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$ is i.i.d. Gaussian noise used to corrupt $\mathbf{x}_0$, and $D_\theta$ is a diffusion transformer \citep{peebles2023scalable}
that learns to recover the clean sample given the corrupted one. $D_\theta$ conditions on $\mathbf{c}$ via cross-attention and on $\sigma$ via adaptive layer normalization \citep{perez2018film, peebles2023scalable}.
The Wan2.1 tokenizer compresses a video sequence of size $(1 + T) \times H \times W \times 3$ into a latent sequence of size $(1 + T') \times H' \times W' \times 16$, where $T'=\frac{T}{4}$, $H'=\frac{H}{8}$, $W'=\frac{W}{8}$; these resulting latent frames compose $\mathbf{x}_0$ above.
The first frame undergoes no temporal compression
to allow for conditioning on a single input image. During training, a conditioning mask is used to ensure that the first latent frame corresponding to the input image remains clean (without noise) while subsequent frames are corrupted with noise.



\textbf{MDP formulation and imitation learning.} We frame robotic manipulation tasks as finite-horizon Markov decision processes (MDPs) defined by the tuple $\langle S, A, T, R, H \rangle$, where $S$ is a set of states, $A$ is a set of actions, $T: S \times A \rightarrow \Pi(S)$ is the state transition function, $R: S \times A \rightarrow \mathbb{R}$ is the reward function, and $H \in \mathbb{N}$ is the time horizon, with time steps $t \in \{1, 2, \ldots, H\}$. We train a policy $\pi : S \rightarrow \Pi (A)$ to maximize rewards, using sparse rewards where $R(s_t, a_t) = 0$ for $t < H$ and terminal rewards $R(s_H, a_H) \in [0, 1]$.
We train policies via imitation learning on expert demonstrations containing state-action pairs. Following \citet{zhao2023learning}, all policies predict action chunks---sequences of actions for multiple timesteps---to improve motion smoothness and success rates.








\textbf{World models and value functions. }A \emph{world model} $\hat{T} : S \times A \rightarrow \Pi(S)$ learns to predict the future state given current state and action, approximating the true environment dynamics. The \emph{value function} for a policy $\pi$ at state $s$ represents expected discounted returns from $s$ under $\pi$. It is defined as $V^\pi(s) = \mathbb{E}_{\tau \sim \pi}\left[ \sum_{k=t}^{H} \gamma^{k-t} R(s_k, a_k) \mid s_t = s \right] = \mathbb{E}_{\tau \sim \pi}\left[\gamma^{H-t} R(s_H, a_H) \mid s_t = s \right]$ in the sparse reward setting, where $\gamma$ is a discount factor that backpropagates the terminal reward through time. We simply use Monte Carlo returns in this work, labeling each transition in a rollout with the observed return $\gamma^{H-t} R(s_H, a_H)$. \rebuttal{Note: To be precise, we acknowledge that the true state is not fully observable, and the world model predicts future \emph{observations} (robot proprioception and camera images). However, for notational simplicity and readability, we opt to use the term ``state'' and treat observations as approximations of the state.}





\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/cosmos_policy_diffusion_sequence_v2_main_version.001.jpeg}
    \caption{\textbf{The latent diffusion sequence of Cosmos Policy.} We illustrate \emph{latent frame injection}---the primary mechanism for adapting the pretrained Cosmos-Predict2 into a policy that can predict robot actions, future states, and values without architectural changes. First, raw images are tokenized into latent frames (first row). Then, additional modalities are inserted directly into the latent frame sequence of the video diffusion model (second row). The model is then tasked to denoise the noised latent frames conditioned on the clean frames (third row). See Section \ref{sec:latent_injection} for more details. (Note: For simplicity, this figure does not depict certain implementation details; see Figure \ref{fig:cosmos_policy_latent_diffusion_sequence_detailed_version} for a more detailed visualization.)
    }
    \label{fig:cosmos_policy_latent_diffusion_sequence}
    \vspace{-0.2cm}
\end{figure*}


\section{Cosmos Policy: Adapting Video Model for Control \& Planning}

In this section, we discuss how to adapt Cosmos-Predict2 into a unified model that predicts actions, future states, and values. We also discuss leveraging policy rollout data to enable effective planning.

\subsection{Latent Frame Injection: Incorporating New Modalities}
\label{sec:latent_injection}

The original Cosmos-Predict2 model takes as input an image and a textual description to generate a short video for a single camera view. It does not support robot proprioception as input, robot actions or state values as output, nor multiple camera views---all of which are desired or required for manipulation policies.

Rather than designing new model components or making architectural modifications as done in prior works, we propose to encode additional modalities as new latent frames that are directly injected into the video model's latent diffusion sequence. Given a $(1 + T') \times H' \times W' \times 16$ sequence of latent frames, which originally correspond to images in a video, we interleave new modalities (robot state, action chunk, and state values) and images from additional camera views by inserting new latent frames between existing image latent frames. For multiple camera viewpoints, the process is simpler: we simply insert the additional camera images at the image sequence level as shown in the top row of Figure \ref{fig:cosmos_policy_latent_diffusion_sequence}) (and the model is subsequently fine-tuned to handle these additional viewpoints).

We now discuss an illustrative example of latent injection for incorporating new modalities. For a robotic platform with two static third-person cameras and a wrist-mounted camera, our latent sequence contains 11 latent frames: (1) a blank placeholder,\footnote{This is an implementation detail that is necessary due to the VAE's $(1+\frac{T}{4})$ temporal compression scheme discussed in Section \ref{sec:prelim}, which is commonly used in models such as Cosmos-Predict2 and Wan2.1. See Appendix \ref{app:latent_injection_implementation_details} for details.} (2) robot proprioception (e.g., end-effector pose or joint angles), (3) wrist camera image, (4) first third-person camera image, (5) second third-person camera image, (6) action chunk, (7) future robot proprioception, (8) future wrist camera image, (9) future first third-person camera image, (10) future second third-person camera image, and (11) future state value. Among these, (2), (6), (7), and (11) represent new modalities while (3), (5), (8), and (10) represent additional camera views (assuming that the first third-person camera is the ``primary'' camera). To encode the new modalities as latent frames, we fill each $H' \times W' \times C'$ latent volume with normalized and duplicated copies of the robot proprioception, action chunk, or value (where normalization simply consists of rescaling to $[-1,+1]$). See Figure \ref{fig:cosmos_policy_latent_diffusion_sequence} for an illustration. This ordering of modalities in the sequence represents $(s, a, s', V(s'))$, and it allows for autoregressive decoding of actions, future state, and future state value from left to right (see Section \ref{sec:joint_objectives} for further discussions on this).
Note that $s$ and $s'$ only consist of the observations at time $t$ and $t+K$, respectively, where $K$ is the action chunk size. In other words, we do not use input history nor predict future frames across multiple subsequent timesteps. Lastly, latent injection is flexible and can be adapted for any particular robot setup: for example, for a robot with only one third-person camera, one can simply remove the latent frames corresponding to additional camera viewpoints, and this would result in only seven total latent frames.

\rebuttal{See Figure \ref{fig:cosmos_policy_latent_diffusion_sequence_detailed_version} for a more detailed version of Figure \ref{fig:cosmos_policy_latent_diffusion_sequence}. Implementation details for latent injection are provided in Appendix \ref{app:latent_injection_implementation_details}.}


\subsection{Joint Training of Policy, World Model, \& Value Function}
\label{sec:joint_objectives}

\textbf{Implementing joint training objectives.} Now that we have a latent diffusion scheme that incorporates additional modalities and camera views that are compatible with robotic policy learning, we can adapt the video model into a policy by training on robot data. For each training step, we sample a batch of $(s, a, s', V(s'))$
tuples.\footnote{We technically sample the empirical return $G_t=\gamma^{H-t}R(s_H,a_H)$ but write ``$V(s')$'' for notational simplicity and readability.} 50 percent of the batch is sampled from the \emph{demonstrations dataset} and is used to train the policy ($p(a, s', V(s') | s)$), while the other 50 percent is sampled from the \emph{rollouts dataset} and is split into two halves: one half for training the world model ($p(s', V(s') | s, a)$) and the other half for training the value function ($p(V(s') | s, a, s')$). The conditioning scheme---i.e., which part of the latent diffusion sequence is used as conditioning and which part is used as the target to generate---determines which of these three functions is being trained (see Figure \ref{fig:balanced_batches} for more details). Initially, the rollouts dataset is simply a superset of the demonstrations dataset that also includes failed demonstrations, if they exist. (Failed demonstrations are those that do not successfully complete the task when replayed in the environment due to human error during data collection, e.g., in the LIBERO and RoboCasa simulations where roughly 10 to 20 percent of demonstrations fail when replayed. In certain environments where teleoperation data is collected more carefully, such as our real-world ALOHA environment, failed demonstrations do not exist; in this case, the demonstrations dataset and rollouts dataset are equal.)

Note that policy and world model training involves auxiliary targets, i.e., the policy is trained to model not just $p(a|s)$ but rather $p(a, s', V(s') | s)$, and the world model learns not just $p(s' | s, a)$ but rather $p(s', V(s') | s, a)$. We find in Section \ref{sec:direct_polcy_experiments} that the auxiliary supervision improves policy performance. Also, note that the $V(s')$ predictions are conditioned on the full latent prefix (i.e., all of $(s, a, s')$) during initial Cosmos Policy training. However, when we later fine-tune this base checkpoint on policy rollout data to produce a model with more accurate future state and value predictions, we can choose to condition the value generation on a subset of $(s, a, s')$ via input masking. The choice of the input mask determines whether the value function represents the state value $V(s')$ or state-action value $Q(s,a)$; we compare these variations in planning experiments (Section \ref{sec:planning_experiments}).




\textbf{Parallel vs. autoregressive decoding.} Since Cosmos Policy learns to both jointly and conditionally predict the targets $(a, s', V(s'))$ based on apportioned training samples, it can generate actions, future states, and values either jointly in parallel or autoregressively from left to right. Parallel decoding offers greater speed, while autoregressive decoding may provide higher-quality predictions and allow for separate checkpoints to be used for the policy versus the world model and value function. For direct policy evaluation without planning, only the actions are required for task execution, while the latter two outputs can be discarded. Therefore, we use parallel decoding in this case. For evaluations with planning, we enable autoregressive decoding for higher-quality future state and value predictions.


\subsection{Planning with Cosmos Policy's World Model and Value Function}
\label{sec:planning}


Cosmos Policy can be deployed as (1) a direct policy without planning or (2) a planning policy using future state and value predictions to search for higher-quality actions. However, training on demonstrations alone is insufficient for effective planning since the data only covers successful outcomes,\footnote{Some demonstration datasets, including the LIBERO simulation benchmark training set, includes suboptimal behaviors that may not lead to success when replayed in the training environment, due to human errors during teleoperation. Typically, however, most (if not all) trajectories in a demonstration dataset used for imitation learning are successful.} which means that the world model and value function see a narrow state-action distribution and may struggle to generalize beyond that distribution. We thus find it critical to collect policy rollout data and learn from these experiences.



\textbf{Learning from rollout experiences.} We collect rollout data by deploying Cosmos Policy in diverse initial conditions and recording the trajectory as well as the episode outcome (success/fail or a fractional score). Given the rollout dataset, we fine-tune our Cosmos Policy checkpoint, with heavier weighting on the world model and value function predictions: 90 percent of each training batch is split evenly between training the world model and value function, while only 10 percent is used to train the policy.



Once we have the fine-tuned checkpoint for refined world modeling and policy learning, we propose \emph{dual deployment}: the original Cosmos Policy checkpoint serves as the policy (we thus call it the ``policy model''), while the refined checkpoint serves as the world model and value function (we thus call it the ``planning model''). This ensures that the refined world model and value function are trained on on-policy data collected by the original policy.

\textbf{Model-based planning.} Given the policy model and the planning model, we implement best-of-N sampling as follows: (1) sample multiple action proposals from the policy, (2) use the planning model to predict the future state and value for each proposal, (3) select and deploy the action that leads to the predicted state with the highest predicted value.
For greater accuracy and better modeling of potentially multimodal future state and value distributions, we ensemble the predictions by querying the world model three times per action and the value function five times per future state, resulting in fifteen total value predictions for each action proposal. We aggregate these via ``majority mean'': we determine whether the majority predict success or failure (via a fixed threshold) and then average values within the majority group. This approach is more robust to outliers than naive averaging when value predictions are bimodal or exhibit high variance.

To speed up the search process, we use parallelized inference, using N GPUs in best-of-N sampling. We also execute the full action chunk (rather only part of it, as done in receding-horizon control) to avoid further increases in computational cost.



\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/cosmos_policy_aloha_rollouts.001.jpeg}
    \caption{\textbf{Cosmos Policy in the ALOHA robot tasks.} Cosmos Policy can successfully execute real-world robotic control tasks that require long-horizon, high-precision manipulation and have high action multimodality.
    }
    \label{fig:cosmos_policy_aloha_rollouts}
    \vspace{-0.5cm}
\end{figure*}

\section{Experiments}
\label{sec:experiments}







We evaluate Cosmos Policy to answer four key questions: \textbf{(Q1)} How does Cosmos Policy compare with state-of-the-art imitation learning policies when used as a direct policy? \textbf{(Q2)} How important are different components of Cosmos Policy? \textbf{(Q3)} Can Cosmos Policy leverage rollout experiences and learn an accurate world model and value function for effective planning? \textbf{(Q4)} Is it more effective to search using a world model and state value function or a Q-value function (a model-free variation)? We answer these through simulated and real-world evaluations spanning single-arm and dual-arm manipulation tasks.

\subsection{Experimental Setup}

We now describe the three task suites used in our evaluations. Note that further training and evaluation details are available in Appendices \ref{app:training_details} and \ref{app:evaluation_details}.

\textbf{LIBERO simulation benchmark.} The LIBERO benchmark \citep{liu2024libero} consists of a variety of environments and tasks featuring a single Franka Emika Panda robot arm. The four primary task suites include LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-Long (also called LIBERO-10); these assess a policy's ability to handle different spatial layouts, objects, language-specified goals, and long-horizon tasks, respectively. Each task suite provides a training dataset of 500 total demonstrations (10 tasks and 50 demonstrations each). Following \citet{kim2024openvla}, we filter unsuccessful demonstrations for policy training but use the full unfiltered set for world model and value function training.

\rebuttal{\textbf{RoboCasa simulation benchmark.} The RoboCasa benchmark \citep{nasiriany2024robocasa} consists of 24 static kitchen manipulation tasks featuring a single Franka Emika Panda robot arm. We follow the evaluation protocol of several prior works \citep{nasiriany2024robocasa, bjorck2025gr00t, zheng2025flare, han2024dual, jang2025dreamgen, liang2025video}. Specifically, for each task, success rate is evaluated over 50 trials across five evaluation scenes with different floor plans and styles (10 trials per scene), and the average success rate is computed across all 24 tasks over 3 random seeds (3600 trials total). Unlike LIBERO evaluations, the RoboCasa evaluations only consist of \emph{unseen object instances}, and two of the five scenes per task include styles \emph{never encountered in the training data}.}

\rebuttal{The benchmark provides a set of 50 human-teleoperated demonstrations for each task and an additional set of 1000 demonstrations generated via MimicGen  \citep{mandlekar2023mimicgen}, and prior works have shown clear increases in success rates from using larger training datasets \citep{nasiriany2024robocasa, bjorck2025gr00t, zheng2025flare, liang2025video, jang2025dreamgen}. However, to assess the relative data efficiency of Cosmos Policy compared to prior works, we train our method on the 50 human-teleoperated demonstrations alone. Similar to LIBERO data preprocessing, we filter unsuccessful demonstrations for policy training but use the full unfiltered dataset for world model and value function training.}



\textbf{Real-world ALOHA robot tasks.} The ALOHA platform \citep{zhao2023learning} consists of two ViperX 300 S robot arms with three cameras: one top-down and two wrist-mounted. We reduce the controller frequency from 50 Hz to 25 Hz for computational efficiency. All policies take as input robot proprioceptive state (14 joint angles), three camera images, and task descriptions, predicting action chunks of 50 timesteps (2 seconds).\footnote{Diffusion Policy is an exception, as it predicts 48-timestep action chunks since the implementation requires a multiple of 4.} We deploy the full action chunk before requerying the policy.


Our evaluation suite consists of four challenging bimanual manipulation tasks (shown in Figure \ref{fig:cosmos_policy_aloha_rollouts}): (1) ``put X on plate'' (80 demos): place objects on a plate based on language instructions, testing language following; (2) ``fold shirt'' (15 demos): fold one of three T-shirts in multiple steps, testing long-horizon contact-rich manipulation; (3) ``put candies in bowl'' (45 demos): collect scattered candies, testing ability to handle multimodal grasp sequences; and (4) ``put candy in ziploc bag'' (45 demos): open and place items in a ziploc slider bag, testing high-precision manipulation with millimeter tolerance.

The evaluations consist of both in-distribution and out-of-distribution testing conditions, with 101 trials total per method across all tasks. We ensure fair comparison between methods by using the same fixed set of initial states for each method.







\begin{table*}[t]
\centering
\begin{minipage}[t]{0.495\textwidth}
\centering
\caption{\textbf{LIBERO simulation benchmark results.} Success rates (SR) across four LIBERO benchmark task suites \citep{liu2024libero}. Cosmos Policy success rates are averaged over 500 trials for each suite (10 tasks Ã— 50 episodes) and three random seeds (6000 trials total). Our method achieves highest performance overall, even outperforming fine-tuned state-of-the-art vision-language-action (VLA) models.
}
\label{tab:libero_benchmark_results}
\resizebox{\textwidth}{!}{\begin{tabular}{l|c|c|c|c|c}
\toprule
& Spatial & Object & Goal & Long & Average \\
& SR (\%) & SR (\%) & SR (\%) & SR (\%) & SR (\%) \\
\midrule
Diffusion Policy \citep{chi2023diffusion} & 78.3 & 92.5  & 68.3  & 50.5   & 72.4 \\
Dita \citep{hou2025dita} & 97.4 & 94.8 & 93.2 & 83.6 & 92.3 \\
$\pi_0$ \citep{black2024pi_0} & 96.8 & 98.8 & 95.8 & 85.2 & 94.2 \\
UVA \citep{li2025unified} & -- & -- & -- & 90.0 & -- \\
UniVLA \citep{bu2025univla} & 96.5 & 96.8 & 95.6 & 92.0 & 95.2 \\
$\pi_{0.5}$ \citep{intelligence2025pi} & \textbf{98.8} & 98.2 & 98.0 & 92.4 & 96.9 \\
Video Policy \citep{liang2025video} & -- & -- & -- & 94.0 & -- \\
OpenVLA-OFT \citep{kim2025fine} & 97.6 & 98.4 & 97.9 & 94.5 & 97.1  \\
CogVLA \citep{li2025cogvla} & 98.6 & 98.8 & 96.6 & 95.4 & 97.4 \\
\rowcolor{yellow!40} Cosmos Policy (ours) & 98.1 & \textbf{100.0} & \textbf{98.2} & \textbf{97.6} & \textbf{98.5} \\
\bottomrule
\end{tabular}}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\textwidth}
\centering
\caption{\rebuttal{\textbf{RoboCasa simulation benchmark results.} Success rates (SR) across 24 kitchen manipulation tasks \citep{nasiriany2024robocasa}. Cosmos Policy success rates are averaged over 50 trials for each task and three random seeds (3600 trials total). Our method achieves a state-of-the-art average success rate of 67.1\% while requiring significantly fewer training demonstrations (50 versus $>$300).}
}
\label{tab:robocasa_benchmark_results}
\resizebox{\textwidth}{!}{\begin{tabular}{l|c|c}
\toprule
& \# Training Demos per Task & Average SR (\%) \\
\midrule
GR00T-N1 \citep{bjorck2025gr00t} & 300 & 49.6 \\
UVA \citep{li2025unified} & 50 & 50.0 \\
DP-VLA \citep{han2024dual} & 3000 & 57.3 \\
GR00T-N1 + DreamGen \citep{jang2025dreamgen} & 300 (+ 10000 synthetic) & 57.6 \\
GR00T-N1 + DUST \citep{won2025dual} & 300 & 58.5 \\
UWM \citep{zhu2025unified} & 1000 & 60.8 \\
$\pi_0$ \citep{black2024pi_0} & 300 & 62.5 \\
GR00T-N1.5 \citep{bjorck2025gr00t} & 300 & 64.1 \\
Video Policy \citep{liang2025video} & 300 & 66.0 \\
FLARE \citep{zheng2025flare} & 300 & 66.4 \\
GR00T-N1.5 + HAMLET \citep{koo2025hamlet} & 300 & 66.4 \\
\rowcolor{yellow!40} Cosmos Policy (ours) & 50 & \textbf{67.1} \\
\bottomrule
\end{tabular}}
\end{minipage}
\end{table*}





\subsection{Comparing against State-of-the-Art Imitation Policies without Planning}
\label{sec:direct_polcy_experiments}

Here we aim to answer questions \textbf{Q1} and \textbf{Q2} posed in the beginning of this section. We answer \textbf{Q1} by comparing Cosmos Policy as a direct policy (without planning) with state-of-the-art imitation learning policies and assessing their relative effectiveness. We answer \textbf{Q2} by ablating various components of Cosmos Policy and analyzing the resulting effects on task performance.

\textbf{Methods in comparison.} In LIBERO \rebuttal{and Robocasa}, we compare against recent top-performing methods including diffusion-based policies trained from scratch (Diffusion Policy \citep{chi2023diffusion}, Dita \citep{hou2025dita}), video model-based policies (UVA \citep{li2025unified}, \rebuttal{UWM \citep{zhu2025unified}}, Video Policy \citep{liang2025video}), and fine-tuned VLA models ($\pi_0$, $\pi_{0.5}$, OpenVLA-OFT, CogVLA, UniVLA, \rebuttal{DP-VLA \citep{han2024dual}, GR00T-N1.5 \citep{bjorck2025gr00t}}). In real-world ALOHA evaluations, we compare against a competitive subset of policies that have demonstrated strong performance in real-world bimanual manipulation tasks: Diffusion Policy, OpenVLA-OFT+, $\pi_0$, and $\pi_{0.5}$.

\textbf{Results.} Tables \ref{tab:libero_benchmark_results} \rebuttal{and \ref{tab:robocasa_benchmark_results}} show the performance of Cosmos Policy and prior works in LIBERO \rebuttal{and RoboCasa}, respectively, while Figure \ref{fig:aloha_performance_results} shows performance on the ALOHA robot. We find that Cosmos Policy achieves highest overall performance in all three domains, while establishing a new state of the art in the LIBERO \rebuttal{and RoboCasa} benchmarks with 98.5\% \rebuttal{and 67.1\%} average success rates, respectively. These results demonstrate Cosmos Policy's strong multi-task manipulation performance in both in-distribution and out-of-distribution generalization scenarios. In addition, in ALOHA robot evaluations, we find that Cosmos Policy outperforms fine-tuned VLAs $\pi_{0.5}$ and OpenVLA-OFT+---which have been pretrained on large amounts of robotic imitation data---despite not having benefited from similar large-scale action supervision. This finding suggests that video model priors provide a strong initialization for control policies without requiring additional action-labeled robot data. Sample Cosmos Policy rollouts are visualized in Figure \ref{fig:cosmos_policy_aloha_rollouts}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{fig/aloha_task_performance_results_v2.001.jpeg}
    \caption{\textbf{Real-world ALOHA robot evaluation results.} We evaluate state-of-the-art policies on a suite of four tasks and measure the score, which represents average percent completion of each task. Cosmos Policy achieves highest overall score, outperforming all other methods in three of four tasks.
    }
    \label{fig:aloha_performance_results}
    \vspace{-0.5cm}
\end{figure*}



\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/pi05_openvlaoft_aloha_rollouts.001.jpeg}
    \caption{\textbf{Common failure modes of $\pi_{0.5}$ and OpenVLA-OFT+ on two challenging ALOHA robot tasks.} \textbf{Left:} $\pi_{0.5}$ struggles to execute a high-precision grasp and loses grip of the ziploc bag. \textbf{Right:} OpenVLA-OFT+ reaches between two candies rather than towards one, suggesting difficulty with modeling the highly multimodal action distribution.
    }
    \label{fig:pi05_openvlaoft_aloha_rollouts}
    \vspace{-0.5cm}
\end{figure*}



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/cosmos_policy_planning_rollouts.001.jpeg}
    \caption{\textbf{World model predictions: base Cosmos Policy vs. fine-tuned checkpoint.} \textbf{Top:} The base Cosmos Policy's world model may fail to predict errors such as losing grasp of the ziploc bag slider, as it is only trained on demonstrations. \textbf{Bottom:} After fine-tuning on policy rollout data, the world model more accurately predicts the resulting state, enabling more effective planning and eventual episode success.
    }
    \label{fig:cosmos_policy_planning_rollouts}
    \vspace{-0.5cm}
\end{figure*}

Qualitatively, we find that while the fine-tuned VLAs show strong performance on the first two tasks, they encounter difficulties in the last two tasks---``put candies in bowl'' and ``put candy in ziploc bag''---which require handling high action multimodality and executing high-precision grasps, respectively. Figure \ref{fig:pi05_openvlaoft_aloha_rollouts} visualizes two common failure modes of $\pi_{0.5}$ and OpenVLA-OFT+: (1) $\pi_{0.5}$, despite showing highly competitive performance on the first three tasks, struggles to reliably handle the ziploc bag, often missing the initial grasp of the slider with the right arm or not grasping the left side of the bag securely enough with the left arm. (2) OpenVLA-OFT+ often reaches in between two candies rather than directly going for one; we hypothesize that its L1 regression of actions leads to inaccurate modeling of the action distribution in tasks with high multimodality. Compared to these methods, Cosmos Policy handles both high multimodality and high precision with substantially greater reliability.

\textbf{Ablation experiments.} Recall from Section \ref{sec:joint_objectives} that Cosmos Policy's policy and world model training involves additional targets which provide additional supervision: the policy learns to jointly predict $p(a,s',v(s'))|s)$ instead of $p(a|s)$, and the world model learns to jointly predict $p(s', V(s')|s,a)$ instead of $p(s'|s,a)$. To evaluate the effect of these auxiliary learning objectives, we train a version of Cosmos Policy without them by masking the loss on the additional targets. In addition, we assess the importance of the video model priors by training Cosmos Policy from randomly initialized weights. We use the same number of gradient steps as the full policy for both of these variants. As shown in Table \ref{tab:ablations_in_libero}, removing the auxiliary losses leads to a 1.5\% absolute drop in average success rate while training from scratch leads to a 3.9\% drop, suggesting that these components are important for maximal performance. We further evaluate Cosmos Policy trained from scratch on the ALOHA robot for additional supporting evidence and find that it obtains an average score of 80.8 on the ``fold shirt'' task, which is 18.7 points lower than the full Cosmos Policy. Qualitatively, the from-scratch variant exhibits jerky motions that may damage the robot over prolonged deployment, so we halt further evaluations with it. \rebuttal{Additional ablation studies on the Cosmos Policy design and joint training scheme are discussed in Appendix \ref{app:additional_ablation_experiments}.}


\subsection{Evaluations of Cosmos Policy with Model-Based Planning}
\label{sec:planning_experiments}

Here we aim to answer \textbf{Q3} by evaluating Cosmos Policy when deployed with model-based planning (as described in Section \ref{sec:planning}), and \textbf{Q4} by analyzing how the proposed model-based approach compares different variants of planning, such as directly learning a Q-value function without a world model. Since our base Cosmos Policy already obtains high success rates in LIBERO and on the first two ALOHA robot tasks, we focus our study on the last two more challenging ALOHA robot tasks (``put candies in bowl'' and ``put candy in ziploc bag''), where there is more room for improvement. Further, we focus on a more challenging set of initial conditions (difficult in-distribution conditions and OOD conditions) and assess whether planning can enhance performance in these settings.




\textbf{Rollout data collection.} To refine Cosmos Policy's world model and value function predictions and enable more effective planning, we gather a rollout dataset that we use for post-training.
Conveniently, by running the prior direct policy evaluations, we have already aggregated 505 policy rollouts across all policies. Adding to this, we collect 143 more rollouts from Cosmos Policy for the "put candy in ziploc bag" task. The additional episodes are important for this task since training an accurate world model for it is particularly challenging due to low camera observability from the robot's self-occlusion and highly stochastic environment dynamics where even millimeter differences in control can dictate success or failure. We fine-tune the base Cosmos Policy checkpoint on this pool of 648 rollouts to produce a refined ``planning model'' for world modeling and value prediction, as described in Section \ref{sec:planning}.


\textbf{Comparing different value function formulations.} When fine-tuning the base Cosmos Policy checkpoint on the rollout dataset, we use three independent formulations for value function training by using input masks to condition the value predictions on different subsets of inputs: $V(s')$ (mask out $(s, a)$) or $Q(s,a)$ (mask out $s'$). The $V(s')$ variant requires a world model to predict the future state before the value can be estimated, while the $Q(s,a)$ variant enables model-free planning by directly predicting Q-values without future state predictions.


\textbf{Results.} We observe that model-based planning using the $V(s')$ formulation consistently improves success rates over the base Cosmos Policy without planning, as shown in Figure \ref{fig:planning_results}. In ALOHA tasks, we observe a 12.5-point average score increase in the two challenging manipulation tasks which involve multimodal grasp sequences and high-precision manipulation. This is a notable improvement given the limited amount of rollout data available for refining the planning model. Qualitatively, we find that the fine-tuned planning model predicts future states more accurately (see Figure \ref{fig:cosmos_policy_planning_rollouts}) and can plan more effectively, ultimately avoiding making mistakes that the base Cosmos Policy makes, such as losing grasp of the slider while opening the ziploc bag. When comparing model-based ($V(s')$) versus model-free ($Q(s,a)$) planning variants, we observe higher performance with the former, which we attribute its ability to leverage learned environment dynamics for more effective and sample-efficient planning. Given a limited amount of rollout data, we expect difficulty with learning an accurate Q-function and suspect that the model may also overfit given higher input dimensionality.



\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/planning_results.001.jpeg}
    \caption{\textbf{Model-based planning results.} We evaluate the base Cosmos Policy on challenging initial states for the last two ALOHA robot tasks, and compare it with two planning variants (model-based and model-free). We find that the model-based variant ($V(s')$) leads to highest overall performance.
    }
    \label{fig:planning_results}
    \vspace{-0.5cm}
\end{figure*}




\section{Discussion}

We presented Cosmos Policy, a state-of-the-art robot policy fine-tuned from the Cosmos-Predict2 video foundation model that demonstrates strong performance in LIBERO, RoboCasa, and ALOHA robot environments. We also show that incorporating policy rollout data to refine world model and value function predictions enables effective model-based planning. \textbf{Limitations and future work:} We observe substantially lower inference speed when using model-based planning (e.g., around 5 seconds to produce one action chunk), which may limit applicability to dynamic tasks. How to speed up the search is an important direction for future study. In addition, effective planning requires substantial rollout data to achieve accurate predictions beyond the demonstration distribution. Learning from fewer rollouts would increase the accessibility of our approach. Lastly, we focus on best-of-N planning with one layer in the search tree; extending the world model's prediction horizon and planning to greater depths could potentially lead to more effective search. 


\rebuttal{\section{Reproducibility Statement}}

\rebuttal{We release model checkpoints, training data, and code (including training and evaluation scripts) on our project website. Further training and evaluation details are provided in Appendix \ref{app:training_details} and \ref{app:evaluation_details}, respectively.}


\section*{Acknowledgments}

We thank Yu-Wei Chao, Lars Ankile, Alexander Swerdlow, Max Li, and the anonymous reviewers for their constructive comments which have helped improve various elements of this paper. We also thank Dan Blick, Sophia Huang, Mohammad Harrim, Yuzhu Dong, and Pooya Jannaty for their assistance with the OSS release of this project. This work was completed during an internship at NVIDIA in collaboration with Stanford University, and the first author was partially supported by the Robotics and AI Institute.


\bibliography{main_conference}
\bibliographystyle{main_conference}


\newpage

\appendix
\section{Appendix}




\subsection{Latent Injection Implementation
Details}
\label{app:latent_injection_implementation_details}

\rebuttal{As discussed in Section \ref{sec:latent_injection}, Cosmos Policy learns to condition on and generate new non-image modalities (such as actions, robot proprioception, and values) through a mechanism called \emph{latent injection}. Figure \ref{fig:cosmos_policy_latent_diffusion_sequence_detailed_version} provides a detailed visualization of how latent injection operates within Cosmos Policy's latent diffusion sequence.}

\rebuttal{The process begins with a sequence of images from multiple camera viewpoints, along with blank (all-zero) images that serve as placeholders for the new modalities to be injected. Once the video model's VAE tokenizer converts this image sequence into latent frames, we perform latent injection by overwriting the latent frames corresponding to the blank placeholder images with normalized and duplicated copies of the robot proprioception, action chunk, and value. Normalization rescales each modality to the range $[-1,+1]$, while duplication resolves the shape mismatch between the low-dimensional vectors and the target latent volumes.}

\rebuttal{For instance, consider an action chunk with shape $K \times d_{act}$. We first normalize each action dimension to $[-1, +1]$, and then flatten the array into a $(K \times d_{act})$ vector. This vector is duplicated $\frac{(H' \times W' \times C')}{(K \times d_{act})}$ times (where $(H', W', C')$ represents the shape of a single latent frame), reshaped into a $(H' \times W' \times C')$ volume, and used to overwrite the corresponding target latent frame. We apply an analogous process for robot proprioception and value, though their initial shapes differ.}

\rebuttal{Following latent injection, we corrupt portions of the latent sequence by adding randomly sampled Gaussian noise scaled by a randomly sampled noise level (see Appendix \ref{app:noise_distribution_explanation} for details). Cosmos Policy is then trained to denoise these corrupted portions of the latent diffusion sequence while conditioning on the clean (uncorrupted) portions. Although latent injection introduces new modalities that were not present during the base video model's pretraining, we find that the model can still learn to generate them effectively through fine-tuning.}

\rebuttal{At inference time, Cosmos Policy generates clean (denoised) latent frames. Extracting the new modalities involves reversing the latent injection process described above. For example, to extract the action chunk prediction from the generated action latent, we compute the average action chunk across all $\frac{(H' \times W' \times C')}{(K \times d_{act})}$ copies in the latent volume. We then un-normalize the action chunk back to its original scale and deploy either the full action chunk or a portion of it on the robot. On the other hand, extracting the value is much simpler since it is a scalar (represented by a  single floating point number): we simply take the average across the full latent volume and then un-normalize to the original range ($[0,+1]$). Note that extracting non-image modalities like these does not require any VAE decoding since these elements were directly injected into the latent space during training.}



\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/cosmos_policy_diffusion_sequence_v2_detailed_for_appendix.001.jpeg}
    \caption{\textbf{Detailed view of the latent diffusion sequence of Cosmos Policy.} This is a more detailed version of Figure \ref{fig:cosmos_policy_latent_diffusion_sequence} which shows implementation details. As illustrated in the top row, blank (all zero) images are inserted into the input image sequence, and these images are encoded into latent frames by the VAE tokenizer. Then, in the middle row, these placeholder latent frames are completely overwritten by latent injection of new modalities (robot proprioception, actions, and value). Note that a placeholder image is set apart at the beginning of the sequence due to the video model's tokenization scheme, which encodes the first image by itself while the rest of the images are temporally compressed in groups of four (as discussed in Section \ref{sec:prelim}). Therefore, to ensure that current timestep observations and future timestep observations have similarly structured latent representations, we place them after the blank first latent frame in the video diffusion sequence. In addition, since we wish to have one latent frame for each modality and camera viewpoint, we construct four identical copies of each image, as shown in the top row. Each group of four identical images corresponds to a single timestep rather than four timesteps).
    }
    \label{fig:cosmos_policy_latent_diffusion_sequence_detailed_version}
\end{figure*}














\subsection{Training Details}
\label{app:training_details}

\subsubsection{Cosmos Policy Noise Distribution}
\label{app:noise_distribution_explanation}

\rebuttal{\textbf{Changes to noise levels during training and inference.} We find that the base Cosmos-Predict2 model's $\sigma$ (noise level) sampling scheme is suitable for video generation tasks but not the most effective for robot policy training. The latter requires generations to be very precise since the generated actions are used to directly control a robot and small imprecisions can lead to catastrophic failures. Therefore, to improve the accuracy of the generations at test time, we modify the noise level sampling scheme at both train and test time.}

\rebuttal{The base model's original noise distribution is a log-normal distribution, similar to the EDM formulation \citep{karras2022elucidating}: $\text{ln}(\sigma)\sim \mathcal{N}(P_{\text{mean}}, P_{\text{std}}^2)$ where $P_{\text{mean}}=1.39, P_{\text{std}}=1.2$ for Cosmos-Predict2-2B (see Figure \ref{fig:noise_distribution}, left). For action generation, we observe that the low weight on higher noise levels causes inaccurate action predictions during sampling. Diffusion generation begins with pure noise scaled by $\sigma_{\text{max}}=80$ and iteratively denoises over multiple steps as $\sigma$ decreases to $\sigma_{\text{min}}\approx 0$. At each step, the network predicts the noise at the current $\sigma$ level to progressively recover a clean sample. Since the log-normal distribution concentrates training weight at lower noise levels, the model has insufficient signal at the high-$\sigma$ regime where generation begins, causing poor initial denoising and cascading errors. While this may not be critical for image and video generation, we find it is harmful for action generation, where predictions must be precise.}

\rebuttal{Therefore, for Cosmos Policy training, we use a hybrid log-normal-uniform distribution with greater weight on larger noise levels (see Figure \ref{fig:noise_distribution}, right). To implement this, we sample from the original log-normal distribution with probability 0.7 and from a uniform distribution over $[1.0, 85.0]$ with probability 0.3, creating a log-normal distribution with an extended tail at higher $\sigma$ values. We find this empirically improves action prediction accuracy and overall success rate. We chose the 0.7/0.3 split to stay close to the original distribution while extending the high-$\sigma$ tail; these probability values were not tuned.}

\rebuttal{At test time, we find that the final denoising steps with $\sigma \approx 0$ are less accurate than earlier steps at larger $\sigma$ values, likely due to the low signal-to-noise ratio at very small noise levels. Therefore, when sampling from Cosmos Policy, we set a higher lower bound with $\sigma_{\text{min}}=4$ (rather than $\sigma_{\text{min}}=0.002$ as in the original EDM formulation \citep{karras2022elucidating}) while keeping $\sigma_{\text{max}}=80$. This higher lower bound empirically improves prediction accuracy at inference time for actions, future states, and values, as measured by lower L1 loss on training and validation samples.}


\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{fig/noise_schedules_figure.001.jpeg}
    \caption{\rebuttal{\textbf{Base model noise distribution vs. Cosmos Policy's adjusted noise distribution.} We change the base Cosmos-Predict2 video model's log-normal noise distribution (left) into a hybrid log-normal-uniform distribution (right) with more weight on higher noise levels. This modification empirically leads to more accurate generations at test time. See a detailed explanation in Appendix \ref{app:noise_distribution_explanation}.}
    }
    \label{fig:noise_distribution}
\end{figure*}

\subsubsection{LIBERO Training Details}

\rebuttal{We train Cosmos Policy for 40K gradient steps using 64 H100 GPUs with global batch size 1920 (taking 48 hours total). The base Cosmos-Predict2 model is fully fine-tuned (all model weights). We train the policy to predict an action chunk size of 16 timesteps and execute the full chunk at test time before requerying. After 40K gradient steps, the policy's action L1 training loss is 0.012, future proprio L1 training loss is 0.007, future wrist image latent L1 training loss is 0.068, future third-person image latent L1 training loss is 0.036, and value function L1 loss is 0.007. All non-image modalities are normalized to the range $[-1,+1]$ for training. (Note that these are single-step training losses given varying $\sigma$ (noise levels) as input, rather than losses on generations from the multi-step diffusion sampling used during policy inference.) The magnitudes of these losses imply that the model more quickly learns to predict actions, proprioception, and values than images.}

\subsubsection{RoboCasa Training Details}

\rebuttal{We train Cosmos Policy for 45K gradient steps using 32 H100 GPUs with global batch size 800 (taking 48 hours total). The base Cosmos-Predict2 model is fully fine-tuned (all model weights). We train the policy to predict an action chunk of 32 timesteps and execute 16 steps before requerying. After 45K gradient steps, the policy's action L1 training loss is 0.016, future proprio L1 training loss is 0.007, future wrist image latent L1 training loss is 0.084, future third-person images latents L1 training loss is 0.048, and value function L1 loss is 0.007. All non-image modalities are normalized to the range $[-1,+1]$ for training.}

\subsubsection{ALOHA Training Details}

\rebuttal{For Cosmos Policy and all other methods in our ALOHA robot evaluations, we train a single policy on all four tasks combined (185 total demonstrations). Cosmos Policy is trained for 50K gradient steps using 8 H100 GPUs with a global batch size of 200, taking 48 hours total. The base Cosmos-Predict2 model is fully fine-tuned (all model weights). We train with an action chunk size of 50 timesteps (spanning 2 seconds given a 25 Hz controller) and execute the full chunk before requerying. After 50K gradient steps, the policy's action L1 training loss is 0.010, future proprio L1 training loss is 0.008, future wrist images latents L1 training loss is 0.097, future third-person image latent L1 training loss is 0.085, and value function L1 loss is 0.007. All non-image modalities are normalized to the range $[-1,+1]$ for training.}

\rebuttal{For fair comparison with Cosmos Policy, we fine-tune $\pi_{0.5}$, $\pi_0$, and OpenVLA-OFT+ using the same computational budget: 48 hours on 8 H100 GPUs. Despite identical wall-clock time for fine-tuning, differences in training iteration speed across models and training frameworks result in substantial variation in the number of gradient steps completed: $\pi_{0.5}$ and $\pi_0$ are fine-tuned for 400K gradient steps (batch size 256), while OpenVLA-OFT+ is fine-tuned for 32K gradient steps (batch size 96 given gradient accumulation 4). We train for this seemingly large number of gradient steps across all methods because we observe that training loss continues to decrease and task execution quality generally improves with extended training. Finally, for the significantly smaller Diffusion Policy, which only contains approximately 150M parameters (as opposed to 2-7B for the other methods), we train from scratch on 1 H100 GPU for 48 hours, resulting in 190 epochs, equivalent to 72K gradient steps (batch size 350). Like Cosmos Policy, all policies are trained to predict an action chunk of 50 timesteps (except Diffusion Policy, which predicts 48 steps since it requires a multiple of 4), and the full chunk is executed before the model is requeried at test time.} %

\subsection{Evaluation Details}
\label{app:evaluation_details}

\subsubsection{General Cosmos Policy Evaluation Details}

\textbf{Number of denoising steps and parallel vs. autoregressive generation.} When evaluating Cosmos Policy as a direct policy, we predict actions, future state, and future state value altogether in parallel. Since direct policy evaluations do not use planning, we simply discard the future state and value predictions (though these can be saved for visualizations of the policy's intent and estimates of task progress). We use 5 denoising steps for LIBERO and RoboCasa and 10 denoising steps for ALOHA. On the other hand, when evaluating Cosmos Policy with model-based planning, we autoregressively generate actions, then future state, and then future state value. In the real-world evaluations with planning, we use 10 denoising steps for the actions, 5 for the future state, and 5 for the value. In addition, we use an ensemble of 3 future state predictions and an ensemble of 5 value predictions to robustify the estimates.

\subsubsection{Real-World ALOHA Robot Evaluation Details}

\textbf{Number of trials and initial conditions.} We evaluate every method in the real ALOHA robot experiments with 101 total trials: 30 trials for ``put X on plate'', 20 trials for ``fold shirt'', 25 trials for ``put candies in bowl'', and 26 trials for ``put candy in ziploc bag''. For each task, we test on both in-distribution and out-of-distribution (OOD) generalization scenarios with respect to the training demonstration dataset (see Figures \ref{fig:iid_eval_positions} and \ref{fig:ood_eval_positions} for visualizations). We describe the details of the test conditions below:

\begin{itemize}
    \item ``put X on plate'': 20 in-distribution + 10 OOD trials. For in-distribution trials, the two objects (purple eggplant and brown chicken wing) are placed in the same row along the horizontal axis of the table, with varying position along the vertical axis. For OOD trials, the objects are not constrained to be in the same row and the positions are more varied. Max time limit is 350 timesteps (14 seconds).
    \item ``fold shirt'': 12 in-distribution + 8 OOD trials. For in-distribution trials, we use three T-shirts of different colors (blue with white stripes, plain white, and brown camouflage) with four fixed initial positions on the table (back, front, left middle, right middle). These three T-shirts were seen at data collection time (five demos for each of the three shirt, for 15 training demos total). For OOD trials, we use an unseen pink T-shirt with the same four initial positions for the first four trials, and the seen blue-with-white-stripes shirt with unseen distractor objects placed on the table (other shirts folded up and placed on the sides or corners of the table, serving as visual distractors). Max time limit is 1600 timesteps (64 seconds).
    \item ``put candies in bowl'': 15 in-distribution + 10 OOD trials. For in-distribution trials, we place 5 candies in front of a large metal bowl, with a roughly even balance of candies on the left versus right half of the table. For OOD trials, we either (a) place 5 candies in front or behind a large metal bowl with uneven amounts of candies on the left versus right half, or (b) place 5 candies in front or behind an unseen smaller orange bowl. Max time limit is 1100 timesteps (44 seconds).
    \item ``put candy in ziploc bag'': 20 in-distribution trials + 6 OOD trials. For in-distribution trials, we place a pink ziploc bag filled half full with candies in the middle of the table with varying initial position along the vertical axis, and one piece of candy to its right (also varying along the vertical axis). For OOD trials, we replace the pink ziploc bag with an unseen blue ziploc bag that is filled to about 75 percent full (more than seen during training), which requires more firm grasps when grasping the bag so that it does not slip out of grasp as the robot manipulates it. Max time limit is 1100 timesteps (44 seconds).
\end{itemize}



\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/iid_initial_positions.jpeg}
    \caption{\textbf{In-distribution initial conditions for ALOHA robot evaluations.} Here we show sample initial positions used for evaluating policies in conditions similar to the training demonstrations. 
    }
    \label{fig:iid_eval_positions}
    \vspace{-0.2cm}
\end{figure*}


\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/ood_initial_positions.jpeg}
    \caption{\textbf{Out-of-distribution initial conditions for ALOHA robot evaluations.} Here we show sample initial positions used for evaluating policies in unseen test conditions. 
    }
    \label{fig:ood_eval_positions}
    \vspace{-0.2cm}
\end{figure*}



\textbf{Scoring rubrics.} Here we describe how each policy rollout is scored between 0 and 100 points. Each score represents what percent of the task has been successfully completed. We use score instead of success rate since a binary metric does not capture fine-grained details.

\begin{itemize}
    \item ``put X on plate'': 50 points for touching the correct target object. 50 points for putting the correct target object on the plate.
    \item ``fold shirt'': 10 points for each of the following stages: (1) grasped bottom edge of shirt, (2) folded in half, (3) grasped both sleeves, (4) folded sleeves in, (5) grasped bottom edge of shirt, (6) folded in half again, (7) grasped right edge of shirt, (8) folded in half again, (9) released the shirt from grasp, (10) pushed shirt to center of table. In the end, 10 points are deducted if a part of the shirt (e.g. sleeve) sticks out due to an incomplete fold.
    \item ``put candies in bowl'': 20 points for each of 5 candies grasped and placed into the bowl.
    \item ``put candy in ziploc bag'': 20 points for each of the following stages: (1) grasped ziploc bag opener with right arm, (2) grasped ziploc bag's upper-left corner with left arm, (3) opened bag at least halfway by pulling the opener with right arm, (4) grasped candy with right arm, (5) placed candy inside bag. 
\end{itemize}

\rebuttal{\textbf{Detailed in-distribution vs. OOD evaluation scores.} Table \ref{tab:detailed_aloha_results} shows the breakdown of in-distribution and out-of-distribution scores for every method across the ALOHA robot task suite. Cosmos Policy achieves highest aggregate success rates, though $\pi_{0.5}$ shows slightly better performance specifically in OOD test scenarios.}

\begin{table}[h]
\centering
\caption{\textbf{In-distribution vs. OOD evaluation scores for ALOHA robot evaluations.} This table is a a more detailed summary of the results shown in Figure \ref{fig:aloha_performance_results}.}
\label{tab:detailed_aloha_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccc|c}
\toprule
\textbf{Method} & \textbf{``put X on plate'' Score} & \textbf{``fold shirt'' Score} & \textbf{``put candies in bowl'' Score} & \textbf{``put candy in ziploc bag'' Score} & \textbf{Average Score} \\
\midrule
\multicolumn{6}{c}{\textbf{In-distribution}} \\
\midrule
Diffusion Policy & 85.0 & 23.3 & 37.3 & 11.0 & 39.2 \\
OpenVLA-OFT+ & 87.5 & 99.2 & 22.7 & 59.0 & 67.1 \\
$\pi_0$ & 97.5 & 98.3 & 73.3 & 56.0 & 81.3 \\
$\pi_{0.5}$ & 97.5 & 99.2 & 98.7 & 56.0 & 87.8 \\
Cosmos Policy & 100.0 & 99.2 & 100.0 & 86.0 & 96.3 \\
\midrule
\multicolumn{6}{c}{\textbf{OOD}} \\
\midrule
Diffusion Policy & 20.0 & 23.8 & 26.0 & 26.7 & 24.1 \\
OpenVLA-OFT+ & 30.0 & 100.0 & 20.0 & 56.7 & 51.7 \\
$\pi_0$ & 60.0 & 98.8 & 68.0 & 60.0 & 71.7 \\
$\pi_{0.5}$ & 100.0 & 100.0 & 90.0 & 80.0 & 92.5 \\
Cosmos Policy & 100.0 & 100.0 & 74.0 & 83.3 & 89.3 \\
\midrule
\multicolumn{6}{c}{\textbf{Full (In-distribution + OOD)}} \\
\midrule
Diffusion Policy & 63.3 & 23.5 & 32.8 & 14.6 & 33.6 \\
OpenVLA-OFT+ & 68.3 & 99.5 & 21.6 & 58.5 & 62.0 \\
$\pi_0$ & 85.0 & 98.5 & 71.2 & 56.9 & 77.9 \\
$\pi_{0.5}$ & 98.3 & 99.5 & 95.2 & 61.5 & 88.6 \\
Cosmos Policy & 100.0 & 99.5 & 89.6 & 85.4 & 93.6 \\
\bottomrule
\end{tabular}%
}
\end{table}




\subsection{Additional Experiments and Details}

\subsubsection{Additional Ablation Experiments}
\label{app:additional_ablation_experiments}


\begin{table*}[h]
\centering
\caption{\textbf{Cosmos Policy ablations in LIBERO.} Here we report the results of two independent ablations: (1) In Section \ref{sec:joint_objectives}, we discussed that Cosmos Policy's policy and world model training involves additional targets which provide auxiliary supervision: the policy learns to jointly predict $p(a,s',v(s'))|s)$ instead of $p(a|s)$, and the world model learns to jointly predict $p(s', V(s')|s,a)$ instead of $p(s'|s,a)$. To evaluate the effect of this joint learning objective, we train a version of Cosmos Policy without it by masking the loss on the additional targets: the policy predicts only $p(a|s)$ and the world model predicts only $p(s'|s,a)$ (the value function continues to predict $p(V(s')|s,a,s')$ as before). We find that removing the auxiliary objectives leads to a 1.5 point drop in average success rate. (2) We also train a version of Cosmos Policy from scratch and observe a 3.9 point drop from the full version.
}
\label{tab:ablations_in_libero}
\resizebox{0.7\textwidth}{!}{\begin{tabular}{l|c|c|c|c|c}
\toprule
& Spatial & Object & Goal & Long & Average \\
& SR (\%) & SR (\%) & SR (\%) & SR (\%) & SR (\%) \\
\midrule
Cosmos Policy (ours) & 98.1 & 100.0 & 98.2 & 97.6 & 98.5 \\
\quad (1) w/o auxiliary losses & 97.6 & 99.8 & 96.7 & 94.0 & 97.0 \\
\quad (2) w/o pretrained model & 94.7 & 98.9 & 96.3 & 88.6 & 94.6 \\
\bottomrule
\end{tabular}}
\end{table*}



\rebuttal{To further study the effects of individual components of the Cosmos Policy design, as well as the joint training objectives discussed in Section \ref{sec:joint_objectives}, we conduct a series of additional ablation experiments that remove individual components or objectives one at a time, until we ultimately have a barebones policy that only predicts actions (no future state or value). Recall from Section \ref{sec:joint_objectives} that Cosmos Policy is trained with a joint objective that optimizes the policy, world model, and value function altogether in one architecture. Specifically, each batch of training samples is split 50/25/25: 50 percent of the samples are used for training the policy, 25 percent for the world model, and 25 percent for the value function. This balanced splitting determines which parts of the latent diffusion are used as conditioning and which parts are used as the target, which in turn determines whether we are training the policy, the world model, or the value function (see Figure \ref{fig:balanced_batches} for an illustration). In addition to these balanced batches, the policy and world model are trained with auxiliary objectives: the policy learns to jointly predict $p(a,s',v(s'))|s)$ instead of $p(a|s)$, and the world model learns to jointly predict $p(s', V(s')|s,a)$ instead of $p(s'|s,a)$}



\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{fig/balanced_batches_figure.001.jpeg}
    \caption{\textbf{Cosmos Policy balanced batches training scheme.} We illustrate the joint objectives training scheme discussed in Section \ref{sec:joint_objectives}. While each batch of training samples is split 50/25/25 for policy, world model, and value function training, respectively, the full latent diffusion sequence remains fixed, and the conditioning scheme determines which of the three functions is being optimized. During training, the model is tasked to denoise the target noised latent frames conditioned on the clean latent frames. Note that the above depicts the initial base policy training scheme. When optionally refining the world model and value function on policy rollouts (in preparation for model-based planning), we mask out the current state and action during value function training so that the value predictions are only a function of the future state $s'$. Separately, to train a Q-value function, we can instead mask out the future state so that the value predictions are only a function of the current state and action, $(s, a)$; this variant can be used for model-free planning (though we find that the model-based planning variant performs better, as shown in Figure \ref{fig:planning_results}).
    }
    \label{fig:balanced_batches}
    \vspace{-0.2cm}
\end{figure*}



\begin{table*}[h]
\centering
\caption{\rebuttal{\textbf{Cosmos Policy ablations and additional experiments in RoboCasa.} \textbf{Top:} We ablate individual components of the joint objectives training scheme and auxiliary supervision discussed in Section \ref{sec:joint_objectives} and visualized in Figure \ref{fig:balanced_batches}. Each version of the policy is trained with the exact same training hyperparameters and compute as the original Cosmos Policy in RoboCasa, and evaluated across all 24 tasks with 50 trials per task and 3 random seeds (3600 trials total). We remove one component at a time until we arrive at a barebones policy that only predicts actions. Here are details describing each ablation: (1) We ablate value function training samples (last row of Figure \ref{fig:balanced_batches}) and instead use 50\% of batches for policy training and 50\% for world model training. (2) We ablate both world model and value function training samples (last two rows of Figure \ref{fig:balanced_batches}) and instead use 100\% of each training batch for policy training only. (3) Adding to the prior ablation, we also ablate the value target when training the policy, thus optimizing the policy to learn just $\pi(a,s'|s)$ rather than $\pi(a,s',V(s')|s)$. (4) Adding to the prior ablation, we ablate both the future state and value targets when training the policy, thus training a barebones policy that only models $\pi(a|s)$. We observe the most significant drop in performance from the final ablation, suggesting that training the policy to also predict future state is crucial to the effectiveness of Cosmos Policy. \textbf{Bottom:} We also report the performance of Cosmos Policy with just 1 denoising step per action chunk at inference time. We observe a 66.4\% average success rate with just 0.16 second inference latency per action chunk of 32 timesteps on one H100 GPU, which is nearly 4$\times$ faster than the 0.61 second latency observed with 5 denoising steps with only a small hit (-0.5\%) on success rate.}
}
\label{tab:ablations_in_robocasa}
\resizebox{\textwidth}{!}{\begin{tabular}{l|c}
\toprule
& Average SR (\%) \\
\midrule
Cosmos Policy (ours) (5 denoising steps) &  67.1 \\
\quad (1) w/o value function training samples &  66.6 \\
\quad (2) w/o world model and value function training samples &  64.0 \\
\quad (3) w/o world model and value function training samples \& auxiliary value supervision for policy training samples &  62.5 \\
\quad (4) w/o world model and value function training samples \& auxiliary future state and value supervision for policy training samples &  44.4 \\
\midrule
Cosmos Policy w/ 1 denoising step &  66.4 \\
\bottomrule
\end{tabular}}
\end{table*}


\subsubsection{Cosmos Policy Inference Latency}
\label{app:inference_latency}

\rebuttal{Here we discuss measurements of inference latency in terms of wall-clock time. Cosmos Policy generates action, future state, and value predictions in parallel in 0.61 seconds total on 1 H100 GPU when sampling with 5 denoising steps (for LIBERO and RoboCasa). Inference latency increases to 0.95 seconds on 1 H100 GPU when sampling with 10 denoising steps (for ALOHA). Inference latency decreases to 0.16 seconds on 1 H100 GPU when sampling with 1 denoising step. We observe competitive performance in RoboCasa (66.4\% average success rate) even with 1 denoising step, as shown in Table \ref{tab:ablations_in_robocasa}. Note that these latencies are the time spent generating an action \emph{chunk}. For example, on the ALOHA robot, the robot pauses for 0.95 seconds to generate an action chunk that spans 2 seconds of execution.}

\rebuttal{For model-based planning experiments on the real ALOHA robot, we do best-of-N search with $N=8$ using 8 parallel H100 GPUs (1 for each branch in the search). Cosmos Policy first generates $N$ candidate action chunks with 10 denoising steps each, then generates an ensemble of 3 future state predictions per action proposal with 5 denoising steps each, and then generates an ensemble of 5 value predictions per future state prediction with 5 denoising steps each. In total, this search takes 4.9 seconds on 8 parallel H100 GPUs.}

\rebuttal{Though the high inference latency did not prevent effective task execution in our evaluations, we acknowledge the limitations caused by slow inference, such as difficulty in adapting the method to dynamic manipulation tasks or locomotion tasks. Optimizing the inference speed for the policy and the model-based search process would be a promising avenue for future work in order to enable adoption to a wide variety of robotics applications.}

\end{document}
